---
title: "Bellabeat Case Study Report"
author: "Nathan"
date: "2023-07-27"
output: html_document
---

# Step 1: Ask
## Business Task

This analysis was initiated by Urška Sršen, co-founder of Bellabeat. Sršen knows that an analysis of available data on consumers' smart device usage would reveal opportunities for the company to grow, and has provided the following business task:

> Analyse smart device usage data to gain insight into how people are already using smart devices, then generate high-level recommendations for how these insights can inform the marketing strategy for one Bellabeat product.

## Key Stakeholders

* Urška Sršen
  + Co-founder of Bellabeat
  + Initiator of this analysis
* Bellabeat marketing team
  + Intended audience for my presentation
  + Will use my insights to guide marketing strategies

# Step 2: Prepare
## Setting up my tools
### Selecting my tools

For this analysis I wanted a tool or set of tools with the following features:

* Sufficient power enough to handle large data sets, e.g. FitBit data tables with >1M observations
* Functions for data manipulation, e.g. loading, cleaning and combining data sets
* Functions for data analysis, e.g. regression analysis and statistical analysis
* Functions for data visualisation, e.g. plotting
* Methods for storing the details of my analysis methodology separate from the data itself, e.g. separate source code files, as opposed to macros stored within spreadsheet files
* Methods for generating reports from my analysis with as little repetition of work as possible, e.g. inline markdown languages, or Page Layout views in spreadsheet applications
* A straightforward learning process and user interface, e.g. spreadsheet tools typically have a single "Add Chart" tool under which all of their powerful charting options can be found; contrast this with learning to download, enable, and finally use the ggplot2 R library

With these requirements in mind, I considered three tools: R, spreadsheets, and databases:

|Feature|R|Spreadsheets|Databases|
|:------------------------------|:---:|:---:|:---:|
| Power for large data sets     | Yes | No  | Yes |
| Data manipulation tools       | Yes | Yes | Yes |
| Data analysis tools           | Yes | Yes | No  |
| Data visualisation tools      | Yes | Yes | No  |
| Separate analysis files       | Yes | No  | Yes |
| Streamlined report generation | Yes | No  | No  |
| Straightforward to learn      | No  | Yes | No  |

Clearly, R is the best single tool for this analysis. R lacks the straightfoward operation of spreadsheet tools, and I will need to learn libraries and programming techniques as I do my analysis, but this is acceptable given my prior experience with other programming languages like Python. 

Note: Python itself was not considered for this analysis: while it shares most of the features, advantages, and disadvantages of R, I'm already familiar with Python and wanted to use this case study to familiarise myself with R instead

### Setting up my RStudio environment

The first preparation stage involves setting up my RStudio environment for my analysis.

The R chunk below automatically loads all packages included in the "rqd_pkgs" list, installing them first if required: this ensures all packages can be loaded by other analysts replicating my work, and minimises the effort required to modify the package list.

```{r setup, include=FALSE}
# Set up knitting options with the knitr package
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

# Load all required packages
print("Loading packages...")

rqd_pkgs <- c(
  "dplyr",
  "anytime",
  "tidyverse",
  "janitor",
  "readr",
  "skimr",
  "ggplot2",
  "kableExtra",
  "purrr"
)

lapply(rqd_pkgs, function(pkg) {
  if(!requireNamespace(pkg, quietly = FALSE)) {
    cran_mirror <- "https://cran.r-project.org"
    install.packages(as.character(pkg), repos = cran_mirror)
  }
  library(pkg, character.only = TRUE)
})
rm(rqd_pkgs)
print("Loading packages complete.")
```

```{r set_dir, include=FALSE}
# Set the working directory of the R markdown environment to match that of the console
# Omitted from report: for working on my laptop specifically
dir <- "/Users/nathanweaver/Library/CloudStorage/OneDrive-Personal/Documents/Professional/Google Data Cert/Course 8 - Capstone Project/Bellabeat Case Study"
setwd(dir)
cat("Set working directory to:", dir, "\n")
```

## Getting the data

For this data analysis, I'll be making use of one public data set specified by Sršen, plus additional data sets as required to address any limitations found in that dataset.

The data set specified by Sršen is the [FitBit Fitness Tracker Data Set](https://www.kaggle.com/datasets/arashnic/fitbit). This is a public data set available under the [CC0 License](https://creativecommons.org/share-your-work/public-domain/cc/) via [Kaggle user Mobius](https://www.kaggle.com/arashnic).

For the initial analysis, the data set was downloaded in its entirety from Kaggle and stored locally on my computer. This provided a baseline for analysis, with any modifications to file names, folder re-structuring, or removal of unnecessary data tables to be conducted after I'm familiar with the raw data.

### Loading the data

To get a top-level view of the data, I first load all of the required data sets directly into the my R environment. Since all of the required data tables are in CSV format, I use the readr package to iterate over all .csv files in my source data directory, loading them into the environment and naming each new data frame after its source file.

```{r load_data_cleaned}
csv_dir <- "Fitabase_Data_Cleaned"
paths_dfs <- list.files(csv_dir, pattern = "*.csv", full.names = TRUE)

df_names <- paths_dfs %>%
  basename() %>%
  tools::file_path_sans_ext() 

for (i in 1:length(df_names)) {
  assign(df_names[i], read_csv(paths_dfs[i]))
}
```

## Understanding the data
### Overview

The database contains data on the following features of the FitBit devices:

| Feature                 | Units                  | Sampling Rate            |
|:------------------------|:-----------------------|:-------------------------|
| BMI                     | BMI                    | Manual/automatic logging |
| Body Weight             | kg                     | Manual/automatic logging |
| Body Fat                | %                      | Manual/automatic logging |
| Calorie burn            | Calories               | 1 minute                 |
| Distance                | Unknown                | 1 minute                 |
| Heart Rate              | Beats per Minute (BPM) | 5 seconds                |
| Intensity (of exercise) | Factor, 0 to 3         | 1 minute                 |
| METs (during exercise)  | METs                   | 1 minute                 |
| Sleep                   | Factor, 1 to 3         | 1 minute                 |
| Steps                   | Steps                  | 1 minute                 |


Some details of the data gathering are not yet clear to me:

* The real-world meanings of the "Intensity" factor variable levels; presumably 3 is maximum intensity and 0 is inactivity.
* The real-world meanings of the "Sleep" factor variable levels are; presumably it indicates "quality of sleep", with 3 being best.
* What triggers the non-manual reports; presumably it's logged from another device, like a body composition analyser.

### Inconsistent and misleading file names

The file names in the data set present the following issues:

* Unclear if data is source or summarized, e.g. "minuteCaloriesNarrow_merged.csv" is source data, while "dailyCalories_merged.csv" is summarized data
* Summarized data not always tied to specific features, e.g. "dailyActivities_merged.csv" contains data summarized from multiple features: there is no "Activity" feature with an associated source data set.
* Data shape unclear, e.g. "minuteCaloriesNarrow_merged.csv" and "dailyCalories_merged.csv" are both tall data, while "minuteIntensitiesWide_merged.csv" and "dailyIntensities_merged.csv" are both wide data
* Data sampling rate unclear, e.g. "minuteSleep_merged.csv" compared to "daySleep_merged.csv"
* Feature is not first in the file name, affecting file sort operations, e.g. "minuteSleep_merged.csv" is sorted closer to "minuteStepsNarrow_merged.csv" than to the related "daySleep_merged.csv"
* All files are suffixed with "_merged", so no distinction is made by this information and it could be dropped

To correct each of these issues, I'll rename the files using this naming convention:

> [feature]\_[src/sum]\_[interval]\_[shape].[filetype]

For example, "sleepDay_merged.csv" will be renamed to "sleep_sum_days_wide.csv".

### Inconsistent variable names

All variables in the data set are named in CapitalisedCase, whereas I would typically use snake_case by convention. This is a relatively minor issue that I could go without correcting, however a small number of variable names, like logId, are also inconsistently capitalised across different tables. Since I'll be adjusting some names anyway, and there's presumably a library function to change this with minimal effort, I'll put it on the data-cleaning to-do list.

### Inappropriate variable types

Some of the tables in the data set use variables of a type unsuitable for analysis. These variables and their required modifications are tabulated below:

| Variable       | Original Type | Updated Type | Reason |
|:---------------|:--------------|:-------------|:-------|
| ActivityDay    | chr           | datetime     | Cannot perform datetime operations on chr variables |
| ActivityHour   | chr           | datetime     | Cannot perform datetime operations on chr variables |
| ActivityMinute | chr           | datetime     | Cannot perform datetime operations on chr variables |
| Date           | chr           | datetime     | Cannot perform datetime operations on chr variables |
| Id             | num           | chr          | Disable scientific notation and numerical operations (IDs are not a numeric value) |
| LogId          | num           | chr          | Disable scientific notation and numerical operations (IDs are not a numeric value) |
| SleepDay       | chr           | datetime     | Cannot perform datetime operations on chr variables |
| Time           | chr           | datetime     | Cannot perform datetime operations on chr variables |

### Missing context for numeric variables

All of the numeric variables in the data set have clearly defined units except for "Distance". Distance does not appear to have a source data table: it's only included in the "activity_sum_wide_days" and "intensity_sum_wide_days" tables, and only as summary data grouped via Intensity level. Floating-point values between 0 and 1 are present, so it seems reasonable to assume this is either kilometers or miles, as opposed to meters or feet. No geographical information is given in the data set, so I can't assume the participants are from the U.S., where miles would be appropriate. Given miles and kilometers represent the same information on slightly different scales, any insights about different use cases between users should still be apparent, therefore I think it's reasonable to assume the distances are given in kilometers for this analysis.

### Missing context for factor variables

Two variables in the data set, exercise "Intensity" and sleep "Value", are numerical factors with no defined range. For these factors, I can't tell from the data alone whether all possible values that a FitBit can record are present. The values for exercise intensity, for example, range from 0 to 3; it could be the case that the FitBits used only generate four levels of intensity, but it could just as easily be the case that the values go up to 100 (i.e. a percentage). This has clear implications for our analysis: if 3 is the max, records of 3 indicate users wear their FitBits while exercising as hard as they can, whereas if 100 is the max, records of 3 indicate users wear their FitBits while sitting on the couch as hard as they can.

With this in mind, I'm going to invest a bit of time to confirm the meaning of these variables before attempting to analyse them (read: about ten hours to investigate everything _and_ learn how to do it all in R _and_ learn how to make it look suitably pretty and coherent in RMarkdown).

#### Validating "Exercise Intensity" by use of R

I'll start by determining the range of values present in the Intensity data:

``` {r find_intensity_min_max}
cat("Min Intensity:", min(intensity_src_mins_tall$Intensity),
    "\nMax Intensity:", max(intensity_src_mins_tall$Intensity), "\n", sep = "")
```

The "activity_sum_days_wide" table provides some context clues as to what the levels might mean: the table summarises Intensity data into four new variables which, based on their names, appear to be associated with intensity levels like so:

1. Sedentary Minutes
2. Lightly Active Minutes
3. Fairly Active Minutes
4. Very Active Minutes

Let's see if I can confirm this by recreating the data with that naming convention:

```{r compare_intensity_data}
# Generate my version of sleepDay_merged for comparison with the original
intensity_daily_sum_wide <- intensity_src_mins_tall %>%
    mutate(activity_date_floored = floor_date(mdy_hms(ActivityMinute), unit = "days")) %>%
    group_by(Id, activity_date_floored) %>%
    summarize(
        minutes_sedentary      = sum(case_when(Intensity == 0 ~ 1, TRUE ~ 0)),
        minutes_lightly_active = sum(case_when(Intensity == 1 ~ 1, TRUE ~ 0)),
        minutes_fairly_active  = sum(case_when(Intensity == 2 ~ 1, TRUE ~ 0)),
        minutes_very_active    = sum(case_when(Intensity == 3 ~ 1, TRUE ~ 0))
    ) %>%
    mutate(Id_ActivityDate_UID = paste(Id, activity_date_floored, sep = "_")) %>%
    arrange(Id, activity_date_floored, Id_ActivityDate_UID)

# Compare both versions of the data and return any dates with different values
intensity_daily_comp <- activity_sum_days_wide %>%
    mutate(ActivityDate_floored = floor_date(mdy(ActivityDate), unit = "days")) %>%
    mutate(Id_ActivityDate_UID = paste(Id, ActivityDate_floored, sep = "_")) %>%
    with(merge(
        .,
        intensity_daily_sum_wide,
        by = c("Id_ActivityDate_UID"),
        all = TRUE
    )
    ) %>%
    mutate(diff_minutes_sedentary      = minutes_sedentary      - SedentaryMinutes     ) %>%
    mutate(diff_minutes_lightly_active = minutes_lightly_active - LightlyActiveMinutes ) %>%
    mutate(diff_minutes_fairly_active  = minutes_fairly_active  - FairlyActiveMinutes  ) %>%
    mutate(diff_minutes_very_active    = minutes_very_active    - VeryActiveMinutes    ) %>%
    select(
        Id_ActivityDate_UID,
        diff_minutes_sedentary,
        diff_minutes_lightly_active,
        diff_minutes_fairly_active,
        diff_minutes_very_active
    ) %>%
    filter(!(diff_minutes_sedentary == 0 & 
                 diff_minutes_lightly_active == 0 & 
                 diff_minutes_fairly_active == 0 & 
                 diff_minutes_very_active == 0)
    ) %>%
    arrange(Id_ActivityDate_UID)

# For this table, glimpse() shows enough to demonstrate the validity of the method
glimpse(intensity_daily_comp)
```

The approach above appears to work perfectly, with the exception of the "sedentary minutes" calculation, which is consistently higher in my version.

I think its safe to conclude that I got the mapping correct, given that:
a) It's consistently higher by at least 6 hours, which I suspect is caused by my method counting time asleep as "sedentary minutes" - which is not _technically_ wrong, you know - and
b) Reversing the order of the mapping produces entirely wrong results.

That being said, I still don't know if these are the categories FitBits work in, not another naming convention that the data authors came up with, and I don't know if all FitBit models work this way, so let's do something I should have done from the start: read the manuals.

#### Validating "Exercise Intensity" by reading of manuals

Activity trackers like FitBits detect activity intensity partly by measuring the user's heart rate while exercising: a higher heart rate corresponds with a higher degree of exertion. As of April 2016, the three latest FitBit models with heart-rate tracking were:

* FitBit Blaze [(released January 2016)](https://www.youtube.com/watch?v=3k3DNT54NkA)
* FitBit Charge HR [(released January 2015)](https://blog.fitbit.com/charge-hr-and-surge-available-now-plus-new-charge-colors/)
* FitBit Surge [(released January 2015)](https://blog.fitbit.com/charge-hr-and-surge-available-now-plus-new-charge-colors/)

A quick look through the product manuals for each model confirms they all break down user activity into four default heart-rate zones:

|Product|HR Zone 1|HR Zone 2|HR Zone 3|HR Zone 4|
|:------|:----:|:----:|:----:|:----:|
|[FitBit Blaze](https://staticcs.fitbit.com/content/assets/help/manuals/manual_blaze_en_US.pdf)           |"Out of Zone"|"Fat burn"|"Cardio"|"Peak"|
|[FitBit Charge HR](https://staticcs.fitbit.com/content/assets/help/manuals/manual_charge_hr_en_US.pdf)   |"Out of Zone"|"Fat burn"|"Cardio"|"Peak"|
|[FitBit Surge](https://myhelp.fitbit.com/resource/manual_surge_en_US)                                    |"Out of Zone"|"Fat burn"|"Cardio"|"Peak"|

While these aren't exactly the same terms as used in the data set, they're clearly related - "Out of Zone" equates to "Sedentary", for example. 

All three FitBit manuals also make the same claim that the default zones are "based on American Heart Association recommendations". Even without [validating that claim](https://www.heart.org/en/healthy-living/fitness/fitness-basics/aha-recs-for-physical-activity-in-adults), it indicates to me that the reasoning behind each zone is not arbitrary, and is consistent across devices, so I think I can assume any other FitBit models circa 2016 would follow the same classification scheme.

At this point, I'm satisfied that the below are the only four intensity levels I need to consider when analysing the data set, regardless of what models of FitBits were being used:

0. Sedentary Minutes
1. Lightly Active Minutes
2. Fairly Active Minutes
3. Very Active Minutes

#### Validating "Sleep Quality" by use of R _and_ reading of manuals

As with exercise intensity, I start by determining the range of values present in the data:

``` {r find_sleep_min_max}
cat("Min Sleep:", min(sleep_src_mins_tall$value),
    "\nMax Sleep:", max(sleep_src_mins_tall$value), "\n", sep = "")
```

The values for sleep quality range from 1 to 3. The summary data tables for sleep quality introduce only two new variable names: "Total Time In Bed", and "Total Minutes Asleep". There's not a valid name for each level of factor like there was for exercise, so in this case we go straight back to the manuals:

* Blaze: tracks "both your time spent asleep and your sleep quality"
* Charge HR: tracks "the hours you sleep and your movement during the night"
* Surge: tracks "the hours you sleep and your movement during the night"

Further poking around the FitBit help pages on [how to track sleep stats](https://help.fitbit.com/articles/en_US/Help_article/1314.htm) and [what they all mean](https://help.fitbit.com/articles/en_US/Help_article/2163.htm) reveals that different devices track slightly different data if they have heart rate tracking:

* No HR tracking: Generic sleep quality tracking with "Time spent awake, restless, and asleep" categories
* HR tracking: Sleep stage tracking with "Light Sleep, Deep Sleep, and REM Sleep" stages

The help pages also single out the Charge HR and the Surge as the only HR-tracking FitBits to _not_ have full sleep stage tracking, leaving the Blaze as the only device from this time period with that feature. Blaze aside, motion-based sleep quality tracking appears to go all the way back to the [FitBit One](https://myhelp.fitbit.com/s/products?language=en_US&p=one). Given this information, it seems fair to assume the following mapping for the sleep data:

* 1: Awake
* 2: Restless
* 3: Asleep

I can confirm this mapping by attempting to recreare duplicate the summary data in sleepDay_merged:

As with the intensity data, I can confirm this by recreating the data with that naming convention:

```{r compare_sleepDay_data}
# Generate my version of sleepDay_merged for comparison with the original
sleep_src_mins_tall_NW <- sleep_src_mins_tall %>%
    # mutate(date_typed = mdy_hms(date)) %>%
    mutate(date_floored = floor_date(mdy_hms(date), unit = "days")) %>%
    # Sum time asleep for each Log ID
    group_by(logId) %>%
    summarize(
        "Id" = min(Id),
        # Associate each Log ID with the latest date recorded under it
        "SleepDay" = max(date_floored),
        "minutes_in_bed"   = n(),
        "minutes_awake"    = sum(case_when(value == 3 ~ 1, TRUE ~ 0)),
        "minutes_restless" = sum(case_when(value == 2 ~ 1, TRUE ~ 0)),
        "minutes_asleep"   = sum(case_when(value == 1 ~ 1, TRUE ~ 0))
    ) %>%
    # Sum time asleep for each date based on SleepDay
    group_by(Id, SleepDay) %>%
    summarize(
        "TotalSleepRecords_2" = n(),
        "TotalMinutesAsleep_2" = sum(minutes_asleep),
        "TotalTimeInBed_2" = sum(minutes_in_bed),
        "TotalMinutesAwake" = sum(minutes_awake),
        "TotalMinutesRestless" = sum(minutes_restless),
    ) %>%
    mutate("Id_SleepDay_UID" = paste(Id, SleepDay, sep = "_")) %>%
    arrange(Id_SleepDay_UID)

# Compare both versions of the data and return any dates with different values
sleepDay_comp <- sleep_sum_days_wide %>%
    # mutate("SleepDay_typed" = mdy_hms(SleepDay)) %>%
    mutate("SleepDay_floored" = floor_date(mdy_hms(SleepDay), unit = "days")) %>%
    mutate("Id_SleepDay_UID" = paste(Id, SleepDay_floored, sep = "_")) %>%
    arrange(Id_SleepDay_UID) %>%
    with(merge(
        .,
        sleep_src_mins_tall_NW,
        by = c("Id_SleepDay_UID"),
        all = TRUE
    )
    ) %>%
    mutate(recordDiff = TotalSleepRecords_2 - TotalSleepRecords) %>%
    mutate(sleepDiff = TotalMinutesAsleep_2 - TotalMinutesAsleep) %>%
    mutate(bedDiff = TotalTimeInBed_2 - TotalTimeInBed) %>%
    select(
        Id_SleepDay_UID,
        recordDiff,
        sleepDiff,
        bedDiff
    ) %>%
    filter(!(recordDiff == 0 & sleepDiff == 0 & bedDiff == 0)) %>%
    arrange(Id_SleepDay_UID)

# For this table, glimpse() shows enough to demonstrate the validity of the method
glimpse(sleepDay_comp)
```

I was able to recreate the existing sleep_src_mins_tall table almost perfectly by summing sleep times per Log ID, with the latest date associated with each Log ID being used as the "date" for that sleep. In practice it turns out I had the mapping inverted, and actually the following is used:

* 1: Asleep
* 2: Restless
* 3: Awake

So I guess read that as "1 is highest-quality sleep, 3 is worst-quality".

My version of the data contains a few rows that vary slightly from the original, by 1 to 22 minutes. I haven't been able to determine the source of this error, but they're more than close enough to confirm I don't have the factor level mapping backwards, so I'm ready to proceed.

### Duplicate data between tables

The data set includes some tables that contain source data for a given feature, e.g. heart-rate tracking, and others that contain summary data, e.g. everything in the "activity_days_sum_wide" table. 

Some of these are useful, for instance: 

* dailyActivities_merged.csv calculates Sedentary Minutes by excluding time spent asleep. This either requires clever/time-consuming cross-referencing with other tables, or is raw data from a calculation performed on the FitBit itself: either way, I don't want to have to do it again
* sleepDay_merged.csv summarises sleep on a per-night basis, which is more useful for comparing users than the raw, minute-by-minute sleep data

Others are not useful, for instance: 

* calories_sum_mins_wide.csv, intensity_sum_mins_wide.csv, and steps_sum_mins_wide.csv all just pivot minute-level data into one 60-column row per hour containing the same data
* minuteIntensitiesWide_merged.csv just sums and averages intensity per hour

Those tables that do not provide useful summaries can be excluded from the data analysis: if a specific need is found for their data, they can be reloaded or recreated manually as required.

### Duplicate data within tables

The "bodycomp_logs_src_wide.csv" file contains both kilogram and pound variables: these describe the same information, and all of the observations contain values for both variables, so one variable can be dropped with no loss of data. The choice between the two formats seems arbitrary for my analysis, so I'm choosing to keep the kilos data as its expressed in an SI unit.

intensity_sum_hours_wide.csv contains Total Intensity and Average Intensity. Total intensity values exceed 4, the maximum for intensity, and so are not actually useful. Average Intensity is just the Total Intensity for each hour divided by 60 minutes per hour.

# Step 3: Clean
## Cleaning Checklist

## Update file names

For this analysis, I'll rename the files to use this naming convention:

> [feature]\_[src/sum]\_[interval]\_[shape].[filetype]

Applying the naming convention to the data set yields the following file names:

| Original                           | Updated                        |
|:-----------------------------------|:-------------------------------|
| dailyActivity_merged.csv           | activity_sum_days_wide.csv     |
| dailyCalories_merged.csv           | calories_sum_days_tall.csv     |
| dailyIntensities_merged.csv        | intensity_sum_days_wide.csv    |
| dailySteps_merged.csv              | steps_sum_days_tall.csv        |
| heartrate_seconds_merged.csv       | heartrate_src_seconds_tall.csv |
| hourlyCalories_merged.csv          | calories_sum_hours_tall.csv    |
| hourlyIntensities_merged.csv       | intensity_sum_hours_wide.csv   |
| hourlySteps_merged.csv             | steps_sum_hours_tall.csv       |
| minuteCaloriesNarrow_merged.csv    | calories_src_mins_tall.csv     |
| minuteCaloriesWide_merged.csv      | calories_sum_mins_wide.csv     |
| minuteIntensitiesNarrow_merged.csv | intensity_src_mins_tall.csv    |
| minuteIntensitiesWide_merged.csv   | intensity_sum_mins_wide.csv    |
| minuteMETsNarrow_merged.csv        | mets_src_mins_tall.csv         |
| minuteSleep_merged.csv             | sleep_src_mins_tall.csv        |
| minuteStepsNarrow_merged.csv       | steps_src_mins_tall.csv        |
| minuteStepsWide_merged.csv         | steps_sum_mins_wide.csv        |
| sleepDay_merged.csv                | sleep_sum_days_wide.csv        |
| weightLogInfo_merged.csv           | bodycomp_src_logs_wide.csv     |

The conversion was performed manually on my local device.

```{r glimpse_data, include=FALSE, eval=FALSE}
# Glimpse all data frames for a quick overview of the data set
# Omitted from report and evaluation: for personal viz purposes only
for (name in df_names) {
    df <- get(name)
    cat("\n", "Dataframe: ", name, "\n", sep = "")
    str(df)
}
```

## Null data: Did you search for NULLs using conditional formatting and filters

```{r clean_nulls}
# Check for NULLs
cat("Checking for NULL/empty values...\n", sep="")
for (df_name in df_names) {
  df <- get(df_name)
  num_nulls <- sum(is.na(df))
  if(num_nulls) {
    cat(df_name,": ",num_nulls," NULLs","\n",sep="")
  }
  # Check for empty strings (which do not show up as NULLs)
  empty_strings <- df %>%
    filter(if_any(where(is.character), ~ nchar(.) == 0))
  num_empty_strings = nrow(empty_strings)
  if(num_empty_strings) {
    cat(df_name,": ",num_empty_strings," empty strings","\n",sep="")
  }
}
cat("Checking for NULL/empty values done.\n", sep="")
rm(df,num_nulls,empty_strings,num_empty_strings,df_name)
```
## Clean and update variable names

```{r update_variable_names}

# Function Declarations ----

rename_df_variables <- function(df_name, var_mods) {
  cat("DEBUG\tRenaming variables in \"",df_name,"\"...\n", sep = "")
  df <- get(df_name)
  # Check each var name requiring correction against the var names in the df
  for (i in 1:nrow(var_mods)) {
    var_old = var_mods$var_old[i]
    if (!(var_old %in% colnames(df))) {
      next
    }
    # If found, make sure the conversion is applicable to this or all dfs
    tbl <- var_mods$tbl[i]
    if (tbl != "" && tbl != df_name) {
      next
    }
    # Perform the conversion if all checks passed
    var_new = var_mods$var_new[i]
    cat("DEBUG\tdf: ",df_name, "\tvar_old: ",var_old,"\t",sep="")
    cat("var_new: ",var_new,"\t", sep="")
    cat("tbl: ",tbl,"    ", sep="")
    cat("Replacing... ", sep = "")
    df <- df %>% rename(!!var_new := !!var_old)
    cat("Done.\n", sep = "")
  }
  cat("DEBUG\tRenaming variables in \"",df_name,"\" complete.\n", sep = "")
  return(df)
}

# Global Variable Declarations ----

var_mods <- data.frame(
  var_old = character(0),
  var_new  = character(0),
  type_new = character(0),
  tbl = character(0)
)

# WARNING: Ensure table-specific modifications (tbl != "") are positioned above non-specific modifications with matching var_old/var_new values: only the first modification in the list will be applied to matching variables.
# TODO: Eliminate this issue by modifying code to warn/handle conflicting rows

var_mods <- var_mods %>%
  rbind(.,data.frame(var_old="date",                       var_new="bodycomp_datetime",          type_new="POSIXct", tbl="bodycomp_src_logs_wide")) %>%
  rbind(.,data.frame(var_old="time",                       var_new="heart_rate_second",          type_new="POSIXct", tbl="heartrate_src_seconds_tall")) %>%
  rbind(.,data.frame(var_old="value",                      var_new="heart_rate",                 type_new="",        tbl="heartrate_src_seconds_tall")) %>%
  rbind(.,data.frame(var_old="date",                       var_new="sleep_minute",               type_new="POSIXct", tbl="sleep_src_mins_tall")) %>%
  rbind(.,data.frame(var_old="value",                      var_new="sleep_rank",                 type_new="",        tbl="sleep_src_mins_tall")) %>%
  rbind(.,data.frame(var_old="",                           var_new="activity_hour",              type_new="POSIXct", tbl="")) %>%
  rbind(.,data.frame(var_old="",                           var_new="activity_minute",            type_new="POSIXct", tbl="")) %>%
  rbind(.,data.frame(var_old="",                           var_new="sleep_day",                  type_new="POSIXct", tbl="")) %>%
  rbind(.,data.frame(var_old="",                           var_new="id",                         type_new="character",     tbl="")) %>%
  rbind(.,data.frame(var_old="",                           var_new="log_id",                     type_new="character",     tbl="")) %>%
  rbind(.,data.frame(var_old="activity_date",              var_new="activity_day",               type_new="Date",     tbl="")) %>%
  rbind(.,data.frame(var_old="fairly_active_distance",     var_new="distance_fairly_active",     type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="fairly_active_minutes",      var_new="minutes_fairly_active",      type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="light_active_distance",      var_new="distance_lightly_active",    type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="light_active_minutes",       var_new="minutes_lightly_active",     type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="lightly_active_distance",    var_new="distance_lightly_active",    type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="lightly_active_minutes",     var_new="minutes_lightly_active",     type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="logged_activities_distance", var_new="distance_logged_activities", type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="me_ts",                      var_new="mets",                       type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="moderately_active_distance", var_new="distance_moderately_active", type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="sedentary_active_distance",  var_new="distance_sedentary",         type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="sedentary_distance",         var_new="distance_sedentary",         type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="sedentary_active_minutes",   var_new="minutes_sedentary",          type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="sedentary_minutes",          var_new="minutes_sedentary",          type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="step_total",                 var_new="steps_total",                type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="total_distance",             var_new="distance_total",             type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="total_intensity",            var_new="intensity_total",            type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="total_minutes_asleep",       var_new="minutes_asleep_total",       type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="total_sleep_records",        var_new="sleep_records_total",        type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="total_steps",                var_new="steps_total",                type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="total_time_in_bed",          var_new="minutes_in_bed_total",       type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="tracker_distance",           var_new="distance_tracker",           type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="very_active_distance",       var_new="distance_very_active",       type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="very_active_minutes",        var_new="minutes_very_active",        type_new="",        tbl=""))

## Rename Variables ----

cat("DEBUG\tCleaning variable names...\n", sep = "")
for(df_name in df_names) {
  cat("DEBUG\tCleaning ",df_name,"...\n", sep = "")
  assign(df_name, get(df_name) %>% clean_names())
}
cat("DEBUG\tCleaning variable names complete.\n", sep = "")

cat("DEBUG\tRenaming variables...\n", sep = "")
var_mods_rename <- var_mods %>%
  filter(var_old != "" & var_new != "")
for(df_name in df_names) {
  assign(df_name, rename_df_variables(df_name, var_mods_rename))
}
cat("DEBUG\tRenaming variables complete.\n", sep = "")
```

## Mismatched data types: Did you check that numeric, date, and string data are typecast correctly?

| Variable        | Original Type | Updated Type | Reason                                                           |
|:----------------|:--------------|:-------------|:-----------------------------------------------------------------|
| activity_day    | chr           | datetime     | Cannot perform datetime operations on chr variables              |
| activity_hour   | chr           | datetime     | Cannot perform datetime operations on chr variables              |
| activity_minute | chr           | datetime     | Cannot perform datetime operations on chr variables              |
| date            | chr           | datetime     | Cannot perform datetime operations on chr variables              |
| id              | num           | chr          | Disable numerical operations (IDs are considered a UID string)   |
| log_id          | num           | chr          | Disable numerical operations (IDs are considered a UID string)   |
| time            | chr           | datetime     | Cannot perform datetime operations on chr variables              |
| sleep_rank      | num           | factor 1:3   | Disable numerical operations (value is a ranking, not an amount) |
| intensity       | num           | factor 0:3   | Disable numerical operations (value is a ranking, not an amount) |

```{r update_variable_types_nonfactors}

# Global Variable Declarations ----

var_mods_recast <- var_mods %>%
  filter(type_new != "")

# Function Declarations ----

are_identical_lists <- function(list1, list2) {
  if (length(list1) != length(list2)) {
    return(FALSE)
  }
  for (i in seq_along(list1)) {
    if(!identical(list1[[i]], list2[[i]])) {
      cat("Non-identical lists at list1[",i,"]. Exiting.\n", sep = "")
      print(list1[[i]])
      print(list2[[i]])
      return(FALSE)
    }
  }
  return(TRUE)
}

get_df_var_types <- function(df_name) {
  cat("DEBUG\tGetting current variable types for ", df_name, "... ", sep = "")
  df <- get(df_name)
  var_types <- data.frame(
    var = names(df),
    type = sapply(df, function(col) class(col)[1])
  )
  cat("Done.\n", sep = "")
  return(var_types)
}

get_df_target_var_types <- function(df_name) {
  cat("DEBUG\tGetting target variable types for ", df_name, "...\n", sep = "")
  var_types <- get_df_var_types(df_name)
  # Iterate over rows in current var_types
  for (var_row in 1:nrow(var_types)) {
    # Check if var name is in var_mods_recast
    var_name <- var_types$var[var_row]
    for(mods_row in 1:nrow(var_mods_recast)) {
      var_name_mods <- var_mods_recast$var_new[mods_row]
      # If no, skip: if yes, replace type with target
      if(var_name == var_name_mods) {
        type_new <- var_mods_recast$type_new[mods_row]
        cat("DEBUG\tUpdated \"", var_name, "\" from ", var_types$type[var_row], " to ", var_mods_recast$type_new[mods_row], ".\n", sep = "")
        var_types$type[var_row] <- type_new
      }
    }
  }
  cat("DEBUG\tGetting target variable types for ", df_name, " done.\n", sep = "")
  return(var_types)
}

recast_variables <- function(df_name) {
  df <- get(df_name)
  for (i in 1:nrow(var_mods_recast)) {
    var_new <- var_mods_recast$var_new[i]
    type_new <- var_mods_recast$type_new[i]
    if (var_new %in% colnames(df)) {
      cat("DEBUG\tConverting ",df_name,"$",var_new," to ",type_new, "... ", sep = "")
      if (type_new == "character") {
        df <- df %>% mutate("{var_new}" := as.character(!!sym(var_new)))
      } else if (type_new == "Date") {
        df <- df %>% mutate("{var_new}" := mdy(!!sym(var_new)))
      } else if (type_new == "POSIXct") {
        df <- df %>% mutate("{var_new}" := mdy_hms(!!sym(var_new)))
      } else {
        cat("type_new not found: not converting.", sep = "")
      }
      cat("Done.\n", sep = "")
    }
  }
  return(df)
}

# Recast Variables ----

cat("DEBUG\tGenerating list of target column types for testing...\n", sep = "")
df_types_target <-lapply(df_names, get_df_target_var_types)
names(df_types_target) <- df_names
cat("DEBUG\tGenerating list of target column types complete.\n", sep = "")

cat("DEBUG\tRecasting variables...\n", sep = "")
for (df_name in df_names) {
  cat("DEBUG\tRecasting ",df_name,"...\n", sep = "")
  assign(df_name, recast_variables(df_name))
  cat("DEBUG\tConverting ",df_name," complete.\n", sep = "")
}
cat("DEBUG\tRecasting variables complete.\n", sep = "")

# Test Recasting of Variables ----

cat("DEBUG\tGenerating list of updated column types...\n", sep = "")
df_types_after <- lapply(df_names, get_df_var_types)
names(df_types_after) <- df_names
cat("DEBUG\tGenerating list of updated column types complete.\n", sep = "")

cat("DEBUG\tChecking updated column types against target types...\n", sep = "")
test_succeeded <- are_identical_lists(df_types_after, df_types_target)
cat("DEBUG\tChecking updated column types against target types complete.\n", sep = "")
cat("Data recasting ", case_when(test_succeeded ~ "succeeded", TRUE ~"failed"), ".", sep = "")
```

``` {r update_variable_types_factors}

# Individual recast of variables with only one occurrence
str(sleep_src_mins_tall)
sleep_src_mins_tall <- sleep_src_mins_tall %>%
    mutate(sleep_rank = factor(sleep_rank, levels = 1:3, labels = c("Asleep", "Restless", "Awake")))
str(sleep_src_mins_tall)

str(intensity_src_mins_tall)
intensity_src_mins_tall <- intensity_src_mins_tall %>%
    mutate(intensity = factor(intensity, levels = 0:3, labels = c("Sedentary", "Lightly Active", "Fairly Active", "Very Active")))
str(intensity_src_mins_tall)

```

Given the large number of variables in the data set, the recasting procedure includes test code to confirm the updated variable types match the types specified in the variable mods list. The test code works by first generating a list of the desired final variable/type pairs, then, once the conversion is completed, generating a second list of the actual variable/type pairs in the data frames to compare it to. This has the advantage of not just confirming the desired conversions took place, but also checks for any unintended changes to variables that did not require conversion.

Building the test code added a decent amount of work to the project, but now I have it working, it can be reused and scaled to future projects.

## Messy (inconsistent) date formats: Did you format the dates consistently throughout your dataset?

This step was completed as part of the previous step when date values were typecast.

## Misspelled words: Did you locate all misspellings?

### Checking the variable names

There's no magic here in terms of variable names: I just finished updating/cleaning up the variable names myself, so this step is just a quick re-print and manual spell-check of all of the variable names in the data set.

```{r clean_misspellings}
for(df_name in df_names) {
  print(df_name)
  glimpse(get(df_name))
}
```

### Checking string-type variables

The only non-numeric string-type data in the data set are the factor variables I defined myself: these were spell-checked and found to be correct.

## TODO: Mistyped numbers: Did you double-check that your numeric data has been entered correctly?
### ID lengths are correct

Valid person ID values are ten-digit numbers, whereas valid Log ID values are eleven-digit based on the "sleep_src_mins_tall" data frame. I wrote a short function to validate the length of all rows in a data frame for a given column number and valid length.

```{r validate_ID_lengths}
validate_string_length <- function(df_name, col_name, valid_length) {
  cat("Checking ",df_name," for invalid ",col_name," values... ",sep="")
  df <- get(df_name)
  if (col_name %in% colnames(df)) {
    invalid_values <- df %>% select(!!sym(col_name)) %>% filter(nchar(!!sym(col_name)) != valid_length)
  }
  if (exists('invalid_values') && nrow(invalid_values) > 0) {
    cat(nrow(invalid_values)," invalid values found:\n",sep="")
    glimpse(invalid_values)
    rm(invalid_values)
  } else {
    cat("complete.\n",sep="")
  }
  rm(df)
}

result <- map(df_names, ~validate_string_length(.x, col_name = "id", valid_length = 10))
result <- map(df_names, ~validate_string_length(.x, col_name = "log_id", valid_length = 11))
rm(result)
```

This check produced 67 invalid values in the "bodycomp_src_logs_wide" data frame. Manual inspection revealed that the log_ID values in this table are all thirteen-digit, not eleven-digit: this is not an error, so the data will not be modified.

### Dates are all within range

The data set is described as containing data from users collected between "03.12.2016-05.12.2016": all records in the data set were validated against this date range.

```{r validate_all_dates}

validate_dates <- function(df_name) {
  valid_dates_stt <- as.Date("2016-03-12", format = "%Y-%m-%d")
  valid_dates_end <- as.Date("2016-05-14", format = "%Y-%m-%d")
  
  cat("Validating dates for ",df_name,"... ",sep="")
  
  date_columns <- get(df_name) %>%
    select_if(function(col) is.POSIXct(col) || is.Date(col))
  
  if (ncol(date_columns) == 0) {
    cat("Error: no date column found.\n",sep="")
  } else if (ncol(date_columns) > 1) {
    cat("Error: more than one date column found for ",df_name,".\n",sep="")
  } else {
    outside_range <- date_columns %>%
      filter(if_any(everything(), ~ . < valid_dates_stt | . > valid_dates_end))
    if (nrow(outside_range) > 0) {
      cat("found ",nrow(outside_range)," invalid values:\n",sep="")
      print(outside_range)
      rm(outside_range)
    } else {
      cat("complete.\n")
    }
  }
  rm(date_columns)
}

result <- lapply(df_names, validate_dates)
rm(result)
```

This check found dates just outside the range, dating up to 8am on 05.13.2016, the day after the data set supposedly ended. I didn't consider this to be a problem, so the valid end-date was updated to the 14th of May 2016 accordingly, and all dates passed this check.

### TODO: Values are within appropriate ranges for their units (e.g. non-negative)
  - All numeric values are non-negative
  - Percentage values are less than 100
  - Weight values are positive and make sense (e.g. less than 200kg)
  - BMI values are in range
  - Daily, hourly, and minute duration sums are no more than one day, hour, or minute, respectively
  - Distances make sense
  - Step counts make sense (check for >20000 for a start)
  - Calories are within normal range
  - Heart rates are less than 200
  - 
  
```{r validate_all_numeric}
validate_numerics <- function(df_name) {
  # This function performs all checks on numeric values that are required in more than one data-frame, e.g. non-negativity and summation
  cat("Validating numerics for ",df_name,"... ",sep="")
  numerics <- get(df_name) %>% select_if(is.numeric)
  if (ncol(numerics) == 0) {
    cat("No numeric variables found.\n",sep="")
  } else {
    # Check for negative values
    negative_values <- numerics %>%
      filter(if_any(everything(), ~ . < 0))
    if (nrow(negative_values) > 0) {
      cat("found ",nrow(negative_values)," invalid values:\n",sep="")
      print(negative_values)
    } else {
      cat("complete.\n")
    }
    rm(negative_values)
    # Check for summation
  }
  rm(numerics)
}

result <- lapply(df_names, validate_numerics)
rm(result)
```

Note: The maximum values quoted below are used as thresholds above which values may not be realistic, not as hard limits for acceptability. A heart-rate of 200bpm, for example, is entirely possible, but it is high enough that I would want to check if the data point corresponded to a period of high-intensity exercise.

```{r validate_specific_numeric}
# Generic function definition
# For a given list of dfs, column names, and a min-max range, check all matching columns in all matching dfs against that range
validate_within_range <- function(df_name, column_names, range_min, range_max) {
  df <- get(df_name)
  for (col_name in column_names) {
    cat("Checking ranges for ",df_name,"[",col_name,"]... ",sep="")
    if (!(col_name %in% colnames(df))) {
      cat("column not found.\n")
    } else {
      out_of_range <- df %>%
        filter(!!sym(col_name) < range_min | !!sym(col_name) > range_max)
      if (nrow(out_of_range) <= 0) {
        cat("complete.\n",sep="")
      } else {
        cat("Found ",nrow(out_of_range)," invalid values:\n",sep="")
        glimpse(out_of_range)
      }
      rm(out_of_range)
    }
  }
rm(df)
}

# Minute summation
valid_df_names <- c(
  "activity_sum_days_wide",
  "intensity_sum_days_wide",
  "sleep_sum_days_wide")
valid_col_names <- c(
  "minutes_very_active",
  "minutes_fairly_active",
  "minutes_lightly_active",
  "minutes_sedentary",
  "minutes_asleep_total",
  "minutes_in_bed_total")
range_max <- 60 * 12 # Minutes in half a day
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# Percentages
valid_df_names <- c("bodycomp_src_logs_wide")
valid_col_names <- c("fat")
range_max <- 30 # Arbitrarily chosen as high enough to be a potentially erroneous value
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# Weights (kg)
valid_df_names <- c("bodycomp_src_logs_wide")
valid_col_names <- c("weight_kg")
range_max <- 150 # Arbitrarily chosen as high enough to be a potentially erroneous value
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# Weights (pounds, same limit as for weight in kilos)
valid_df_names <- c("bodycomp_src_logs_wide")
valid_col_names <- c("weight_pounds")
kg2lb <- 2.204623
range_max <- 150 * kg2lb # Arbitrarily chosen as high enough to be a potentially erroneous value
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# BMI 
valid_df_names <- c("bodycomp_src_logs_wide")
valid_col_names <- c("bmi")
range_max <- 40.0 # Corresponds with the WHO "Obese (Class III)" weight category
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# Distances
valid_df_names <- c(
  "activity_sum_days_wide",
  "intensity_sum_days_wide")
valid_col_names <- c(
  "distance_lightly_active",
  "distance_logged_activities",
  "distance_moderately_active",
  "distance_sedentary",
  "distance_total",
  "distance_tracker",
  "distance_very_active")
range_max <- 21.08241 # Equivalent to one half-marathon
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# Step counts
valid_df_names <- c(
  "activity_sum_days_wide",
  "steps_sum_days_tall",
  "steps_sum_hours_tall",
  "steps_src_mins_tall")
valid_col_names <- c(
  "steps",
  "steps_total")
range_max <- 20000 # Chosen arbitrarily as double the typically-recommended daily step count
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# Calories
valid_df_names <- c(
  "activity_sum_days_wide",
  "calories_src_mins_tall",
  "calories_sum_days_tall",
  "calories_sum_hours_tall")
valid_col_names <- c("calories")
range_max  <- 4000 # Chosen arbitrarily as double the typically-recommended daily caloric intake
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# TODO: METs

# Heart Rates
valid_df_names <- c("heartrate_src_seconds_tall")
valid_col_names <- c("heart_rate")
range_max  <- 200 # Chosen based on average 100% heart-rate for a 20 y.o. (Source: American Heart Foundation)
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

rm(result, valid_df_names, valid_col_names)
```

  - All numeric values are non-negative
  - Percentage values are less than 100
  - Weight values are positive and make sense (e.g. less than 200kg)
  - BMI values are in range
  - Daily, hourly, and minute duration sums are no more than one day, hour, or minute, respectively
  - Distances make sense
  - Step counts make sense (check for >20000 for a start)
  - Calories are within normal range
  - Heart rates are less than 200
  - 

Findings:
  
  * There were no invalid negative values
  * There were no invalid percentage values
  * There were no invalid minute summations
  * There were 19 step count data points above 20,000: these were manually checked and found to be associated with long distances covered and high levels of exercise, which made sense, so I considered that data valid.
  * There were a total of four unique data points with a distance covered of more than one half-marathon: all of these also indicatd a very high level of exercise for at lest 90 minutes, which made sense, so I considered the data valid
  * Calories: 21 records showed caloric burns of over 4000 calories. All but one of those were associated with very high levels of activity: the outlier showed only 30 minutes total as "Very Active" or "Lightly Active", compared to 120+ minutes for all other records. It does show 15km distance covered that day, which is nearly a third of a marathon, so for now it makes enough sense to keep it in, and I will analyse it further after the cleaning stage.
  * Heart-rate: 13 records were found with a heart-rate between 200 and 203, all of which corresponded to a single user over a 30-minute period of "Very Active" intensity.
  

Sources for ranges:
  * Fat %: TODO
  * Weight (kg/lb): TODO
  * BMI: https://en.wikipedia.org/wiki/Body_mass_index#Categories
  * Half-marathon: https://en.wikipedia.org/wiki/Half_marathon
  * Step-count: TODO
  * Caloric intake: TODO
  * Max HR: https://www.heart.org/en/healthy-living/fitness/fitness-basics/target-heart-rates

## TODO: Extra spaces and characters: Did you remove any extra spaces or characters using the TRIM function?
## TODO: Duplicates: Did you remove duplicates in spreadsheets using the Remove Duplicates function or DISTINCT in SQL?
## TODO: Messy (inconsistent) strings: Did you make sure that all of your strings are consistent and meaningful?

## TODO: Misleading variable labels (columns): Did you name your columns meaningfully?
## TODO: Truncated data: Did you check for truncated or missing data that needs correction?
## TODO: Business Logic: Did you check that the data makes sense given your knowledge of the business? 

* Update file names
* Clean variable names
* Update variable names
* Update variable types


## TODO: Transform into tidy data frames

Update: There may not be any real point to this step, as it does not actually aid the data analysis. I'll leave it for now unless a need arises

Of all the source data tables, only the body comp data is presented in wide format. The source wide data includes columns for Mass, Fat Percentage, and BMI. These will be split out into individual tall-format data tables for individual analysis

## Null data: Did you search for NULLs using conditional formatting and filters
* Misspelled words: Did you locate all misspellings?
* Mistyped numbers: Did you double-check that your numeric data has been entered correctly?
* Extra spaces and characters: Did you remove any extra spaces or characters using the TRIM function?
* Duplicates: Did you remove duplicates in spreadsheets using the Remove Duplicates function/DISTINCT in SQL?
* Mismatched data types: Did you check that numeric, date, and string data are typecast correctly?
* Messy (inconsistent) strings: Did you make sure that all of your strings are consistent and meaningful?
* Messy (inconsistent) date formats: Did you format the dates consistently throughout your dataset?
* Misleading variable labels (columns): Did you name your columns meaningfully?
* Truncated data: Did you check for truncated or missing data that needs correction?
* Business Logic: Did you check that the data makes sense given your knowledge of the business? 

## TODO: Remove duplicate data

* sleepday(?): Data for Id 4702921684, 5/7/2016, is duplicated.
* bodycomp_logs_src_wide.csv: drop pounds when splitting into tall data tables
* intensity_sum_hours_wide.csv: total intensity not required

## TODO: Exclude unnecessary tables

* intensity_sum_hours_wide.csv
* calories_sum_mins_wide.csv
* intensity_sum_mins_wide.csv
* steps_sum_mins_wide.csv

## TODO: Exclude unnecessary table columns

* all intensity_sum_ tables except intensity_sum_wide_hours

# TODO Step 4: Analyse
In which I do my actual analysis

## Analysing function usage

### Method

This part of the analysis uses the unique IDs to determine what proportion of the sample base uses each feature:
<ol>
  <li>For each data frame:
    <ol>
      <li>Count number of unique IDs in data frame</li>
      <li>Add all unique IDs into id vector</li>
    </ol>
  </li>
  <li>For each data frame:
    <ol>
      <li>Calculate unique ID number as percentage of total unique IDs</li>
    </ol>
  </li>
  <li>Generate bar-graph showing utilisation rate of each feature:
    <ol>
      <li>Shown initially as a proportion for every data frame</li>
      <li>Summarized into proportions for one indicative data frame per feature</li>
    </ol>
  </li>
</ol>


### Implementation

NOOOOOOOOO

I can't just look at which features are used by 33 users, i.e. group by feature then count IDs: I also need to group by ID and count features. This allows us to investigate which users go with which dataset: are the people who don't use MET tracking the same as the people who don't use HR tracking, for instance



```{r, count_unique_IDs_by_feature}
#TODO: Collate databases by feature type

# Note variable names use "Id" instead of "ID" by convention for consistency with database column names
feature_names = c(
  "heartrate_seconds_merged",
  "minuteCaloriesNarrow_merged",
  "minuteIntensitiesNarrow_merged",
  "minuteMETsNarrow_merged",
  "minuteStepsNarrow_merged",
  "sleep_src_mins_tall",
  "weightLogInfo_merged"
)

column_names <- c("name", "Id_count")
unique_Id_counts <- data.frame(matrix(nrow = 0, ncol = length(column_names)))
colnames(unique_Id_counts) <- column_names

unique_Ids <- vector("numeric")

# Determine the number of unique Ids present in each dataframe
for (name in feature_names) {
  df <- get(name)
  unique_Ids_in_df <- unique(df$Id)
  unique_Ids <- unique(c(unique_Ids, unique_Ids_in_df))
  # Extract count of unique IDs for current dataframe
  new_row <- data.frame(name, length(unique_Ids_in_df))
  colnames(new_row) <- column_names
  unique_Id_counts <- rbind(unique_Id_counts, new_row)
}
unique_Id_counts$name <- reorder(unique_Id_counts$name, unique_Id_counts$Id_count, decreasing = TRUE)

```

```{r, viz_unique_IDs_by_feature}
ggplot(unique_Id_counts, aes(name, Id_count)) +
  geom_col() +
  coord_cartesian(ylim=c(0L, length(unique_Ids))) +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  labs(x="Dataframes", y="Unique ID Count", title="Unique ID Counts for Fitabase Dataframes")
```

```{r, count_features_by_unique_ID}


```

```{r, viz_features_by_unique_ID}

```

### Findings
The most used features were:
* Calorie Tracking
* Exercise Intensity Tracking
* Exercise MET Tracking
* Step Tracking

These features were used by all 33 participants in the database. 

The least used features were:
* Weight logging
* Sleep logging
* Heart-rate monitoring





### Note on different FitBit devices

The dataset does not indicate what type of FitBit each participant was using: it is possible the less-used features are unavailable on certain devices. This has a few potential implications:

* Those consumers who don't use all would like to use the features on fully-featured FitBits, but other factors drive them to purchase FitBits with fewer features, e.g. cost
* 

### MET Tracking and why Bellabeat doesn't require it

All of the most-used features from the database are available on the Bellabeat Ivy except MET tracking. The MET (Metabolic Equivalent of Task) is a unit of measurement for energy expenditure, or exertion, intended to aid direct comparison between different types of exercise. The Ivy does not generate MET data, but does track calories burnt, which can also be used as a measure of exertion. I recommend presenting calorie tracking as a suitable alternative when targeting advertising at those customers who use the MET tracking feature.

Note that the two features are nearly equivalent and may not present any meaningful distinction to customers; I will be conducting 

## Analysing the most-used features

In order to develop a targeted marketing strategy for the Ivy, 

### Consumer Behaviors w.r.t. Calorie Tracking vs. MET Tracking

### Display data for first-glance analysis

``` {r, data_display, include=FALSE, eval=FALSE}
selected_Id <- "1503960366"
sleepDay_merged_oneId <- sleepDay_merged %>%
  filter(Id == selected_Id)
  
ggplot(data = sleepDay_merged_oneId) +
  geom_point(
    mapping = aes(
      x = SleepDay,
      y = TotalMinutesAsleep,
      colour = Id
    )
  )

sleep_src_mins_tall_oneId <- sleep_src_mins_tall %>%
  filter(Id == selected_Id)
  
ggplot(data = sleep_src_mins_tall_oneId) +
  geom_point(
    mapping = aes(
      x = date,
      y = value,
      colour = Id
    )
  )
```

## TODO: Analysing step counts vs distance to see who's running


# TODO Step 5: Share

In which I present all of my key findings and their supporting visualisations

# TODO Step 6: Act

In which I summarise my recommendations