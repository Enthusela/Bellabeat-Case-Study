---
title: "Bellabeat Case Study Report"
author: "Nathan"
date: "2023-07-27"
output: html_document
---

# Step 1: Ask
## Business Task

This analysis was initiated by Urška Sršen, co-founder of Bellabeat. Sršen knows that an analysis of available data on consumers' smart device usage would reveal opportunities for the company to grow, and has provided the following business task:

> Analyse smart device usage data to gain insight into how people are already using smart devices, then generate high-level recommendations for how these insights can inform the marketing strategy for one Bellabeat product.

## Key Stakeholders

* Urška Sršen
  + Co-founder of Bellabeat
  + Initiator of this analysis
* Bellabeat marketing team
  + Intended audience for my presentation
  + Will use my insights to guide marketing strategies

# Step 2: Prepare
## Setting up my tools
### Selecting my tools

For this analysis I wanted a tool or set of tools with the following features:

* Sufficient power enough to handle large data sets, e.g. FitBit data tables with >1M observations
* Functions for data manipulation, e.g. loading, cleaning and combining data sets
* Functions for data analysis, e.g. regression analysis and statistical analysis
* Functions for data visualisation, e.g. plotting
* Methods for storing the details of my analysis methodology separate from the data itself, e.g. separate source code files, as opposed to macros stored within spreadsheet files
* Methods for generating reports from my analysis with as little repetition of work as possible, e.g. inline markdown languages, or Page Layout views in spreadsheet applications
* A straightforward learning process and user interface, e.g. spreadsheet tools typically have a single "Add Chart" tool under which all of their powerful charting options can be found; contrast this with learning to download, enable, and finally use the ggplot2 R library

With these requirements in mind, I considered three tools: R, spreadsheets, and databases:

|Feature|R|Spreadsheets|Databases|
|:------------------------------|:---:|:---:|:---:|
| Power for large data sets     | Yes | No  | Yes |
| Data manipulation tools       | Yes | Yes | Yes |
| Data analysis tools           | Yes | Yes | No  |
| Data visualisation tools      | Yes | Yes | No  |
| Separate analysis files       | Yes | No  | Yes |
| Streamlined report generation | Yes | No  | No  |
| Straightforward to learn      | No  | Yes | No  |

Clearly, R is the best single tool for this analysis. R lacks the straightfoward operation of spreadsheet tools, and I will need to learn libraries and programming techniques as I do my analysis, but this is acceptable given my prior experience with other programming languages like Python. 

Note: Python itself was not considered for this analysis: while it shares most of the features, advantages, and disadvantages of R, I'm already familiar with Python and wanted to use this case study to familiarise myself with R instead

### Setting up my RStudio environment

The first preparation stage involves setting up my RStudio environment for my analysis.

The R chunk below automatically loads all packages included in the "rqd_pkgs" list, installing them first if required: this ensures all packages can be loaded by other analysts replicating my work, and minimises the effort required to modify the package list.

```{r setup, include=FALSE}
# Set up knitting options with the knitr package
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

# Load all required packages
print("Loading packages...")

rqd_pkgs <- c(
  "dplyr",
  "anytime",
  "tidyverse",
  "janitor",
  "readr",
  "skimr",
  "tibble",
  "ggplot2",
  "gridExtra",
  "ggpubr",
  "kableExtra",
  "purrr"
)

lapply(rqd_pkgs, function(pkg) {
  if(!requireNamespace(pkg, quietly = FALSE)) {
    cran_mirror <- "https://cran.r-project.org"
    install.packages(as.character(pkg), repos = cran_mirror)
  }
  library(pkg, character.only = TRUE)
})
rm(rqd_pkgs)
print("Loading packages complete.")
```

```{r set_dir, include=FALSE}
# Set the working directory of the R markdown environment to match that of the console
# Omitted from report: for working on my laptop specifically
dir <- "/Users/nathanweaver/Library/CloudStorage/OneDrive-Personal/Documents/Professional/Google Data Cert/Course 8 - Capstone Project/Bellabeat Case Study"
setwd(dir)
cat("Set working directory to:", dir, "\n")
```

## Getting the data

For this data analysis, I'll be making use of one public data set specified by Sršen, plus additional data sets as required to address any limitations found in that dataset.

The data set specified by Sršen is the [FitBit Fitness Tracker Data Set](https://www.kaggle.com/datasets/arashnic/fitbit). This is a public data set available under the [CC0 License](https://creativecommons.org/share-your-work/public-domain/cc/) via [Kaggle user Mobius](https://www.kaggle.com/arashnic).

For the initial analysis, the data set was downloaded in its entirety from Kaggle and stored locally on my computer. This provided a baseline for analysis, with any modifications to file names, folder re-structuring, or removal of unnecessary data tables to be conducted after I'm familiar with the raw data.

### Loading the data

To get a top-level view of the data, I first load all of the required data sets directly into the my R environment. Since all of the required data tables are in CSV format, I use the readr package to iterate over all .csv files in my source data directory, loading them into the environment and naming each new data frame after its source file.

```{r load_data_cleaned}
csv_dir <- "Fitabase_Data_Cleaned"
paths_dfs <- list.files(csv_dir, pattern = "*.csv", full.names = TRUE)

df_names <- paths_dfs %>%
  basename() %>%
  tools::file_path_sans_ext() 

for (i in 1:length(df_names)) {
  assign(df_names[i], read_csv(paths_dfs[i]))
}
rm(i, paths_dfs)
```

## Understanding the data
### Overview

The database contains data on the following features of the FitBit devices:

| Feature                 | Units                  | Sampling Rate            |
|:------------------------|:-----------------------|:-------------------------|
| BMI                     | BMI                    | Manual/automatic logging |
| Body Weight             | kg                     | Manual/automatic logging |
| Body Fat                | %                      | Manual/automatic logging |
| Calorie burn            | Calories               | 1 minute                 |
| Distance                | Unknown                | 1 minute                 |
| Heart Rate              | Beats per Minute (BPM) | 5 seconds                |
| Intensity (of exercise) | Factor, 0 to 3         | 1 minute                 |
| METs (during exercise)  | METs                   | 1 minute                 |
| Sleep                   | Factor, 1 to 3         | 1 minute                 |
| Steps                   | Steps                  | 1 minute                 |


Some details of the data gathering are not yet clear to me:

* The real-world meanings of the "Intensity" factor variable levels; presumably 3 is maximum intensity and 0 is inactivity.
* The real-world meanings of the "Sleep" factor variable levels are; presumably it indicates "quality of sleep", with 3 being best.
* What triggers the non-manual reports; presumably it's logged from another device, like a body composition analyser.

### Inconsistent and misleading file names

The file names in the data set present the following issues:

* Unclear if data is source or summarized, e.g. "minuteCaloriesNarrow_merged.csv" is source data, while "dailyCalories_merged.csv" is summarized data
* Summarized data not always tied to specific features, e.g. "dailyActivities_merged.csv" contains data summarized from multiple features: there is no "Activity" feature with an associated source data set.
* Data shape unclear, e.g. "minuteCaloriesNarrow_merged.csv" and "dailyCalories_merged.csv" are both tall data, while "minuteIntensitiesWide_merged.csv" and "dailyIntensities_merged.csv" are both wide data
* Data sampling rate unclear, e.g. "minuteSleep_merged.csv" compared to "daySleep_merged.csv"
* Feature is not first in the file name, affecting file sort operations, e.g. "minuteSleep_merged.csv" is sorted closer to "minuteStepsNarrow_merged.csv" than to the related "daySleep_merged.csv"
* All files are suffixed with "_merged", so no distinction is made by this information and it could be dropped

To correct each of these issues, I'll rename the files using this naming convention:

> [feature]\_[src/sum]\_[interval]\_[shape].[filetype]

For example, "sleepDay_merged.csv" will be renamed to "sleep_sum_days_wide.csv".

### Inconsistent variable names

All variables in the data set are named in CapitalisedCase, whereas I would typically use snake_case by convention. This is a relatively minor issue that I could go without correcting, however a small number of variable names, like logId, are also inconsistently capitalised across different tables. Since I'll be adjusting some names anyway, and there's presumably a library function to change this with minimal effort, I'll put it on the data-cleaning to-do list.

### Inappropriate variable types

Some of the tables in the data set use variables of a type unsuitable for analysis. These variables and their required modifications are tabulated below:

| Variable       | Original Type | Updated Type | Reason |
|:---------------|:--------------|:-------------|:-------|
| ActivityDay    | chr           | datetime     | Cannot perform datetime operations on chr variables |
| ActivityHour   | chr           | datetime     | Cannot perform datetime operations on chr variables |
| ActivityMinute | chr           | datetime     | Cannot perform datetime operations on chr variables |
| Date           | chr           | datetime     | Cannot perform datetime operations on chr variables |
| Id             | num           | chr          | Disable scientific notation and numerical operations (IDs are not a numeric value) |
| LogId          | num           | chr          | Disable scientific notation and numerical operations (IDs are not a numeric value) |
| SleepDay       | chr           | datetime     | Cannot perform datetime operations on chr variables |
| Time           | chr           | datetime     | Cannot perform datetime operations on chr variables |

### Missing context for numeric variables

All of the numeric variables in the data set have clearly defined units except for "Distance". Distance does not appear to have a source data table: it's only included in the "activity_sum_wide_days" and "intensity_sum_wide_days" tables, and only as summary data grouped via Intensity level. Floating-point values between 0 and 1 are present, so it seems reasonable to assume this is either kilometers or miles, as opposed to meters or feet. No geographical information is given in the data set, so I can't assume the participants are from the U.S., where miles would be appropriate. Given miles and kilometers represent the same information on slightly different scales, any insights about different use cases between users should still be apparent, therefore I think it's reasonable to assume the distances are given in kilometers for this analysis.

### Missing context for factor variables

Two variables in the data set, exercise "Intensity" and sleep "Value", are numerical factors with no defined range. For these factors, I can't tell from the data alone whether all possible values that a FitBit can record are present. The values for exercise intensity, for example, range from 0 to 3; it could be the case that the FitBits used only generate four levels of intensity, but it could just as easily be the case that the values go up to 100 (i.e. a percentage). This has clear implications for our analysis: if 3 is the max, records of 3 indicate users wear their FitBits while exercising as hard as they can, whereas if 100 is the max, records of 3 indicate users wear their FitBits while sitting on the couch as hard as they can.

With this in mind, I'm going to invest a bit of time to confirm the meaning of these variables before attempting to analyse them (read: about ten hours to investigate everything _and_ learn how to do it all in R _and_ learn how to make it look suitably pretty and coherent in RMarkdown).

#### Validating "Exercise Intensity" by use of R

I'll start by determining the range of values present in the Intensity data:

``` {r find_intensity_min_max, eval=FALSE}
cat("Min Intensity:", min(intensity_src_mins_tall$Intensity),
    "\nMax Intensity:", max(intensity_src_mins_tall$Intensity), "\n", sep = "")
```

The "activity_sum_days_wide" table provides some context clues as to what the levels might mean: the table summarises Intensity data into four new variables which, based on their names, appear to be associated with intensity levels like so:

1. Sedentary Minutes
2. Lightly Active Minutes
3. Fairly Active Minutes
4. Very Active Minutes

Let's see if I can confirm this by recreating the data with that naming convention:

```{r compare_intensity_data, eval=FALSE}
# Generate my version of sleepDay_merged for comparison with the original
intensity_daily_sum_wide <- intensity_src_mins_tall %>%
    mutate(activity_date_floored = floor_date(mdy_hms(ActivityMinute), unit = "days")) %>%
    group_by(Id, activity_date_floored) %>%
    summarize(
        minutes_sedentary      = sum(case_when(Intensity == 0 ~ 1, TRUE ~ 0)),
        minutes_lightly_active = sum(case_when(Intensity == 1 ~ 1, TRUE ~ 0)),
        minutes_fairly_active  = sum(case_when(Intensity == 2 ~ 1, TRUE ~ 0)),
        minutes_very_active    = sum(case_when(Intensity == 3 ~ 1, TRUE ~ 0))
    ) %>%
    mutate(Id_ActivityDate_UID = paste(Id, activity_date_floored, sep = "_")) %>%
    arrange(Id, activity_date_floored, Id_ActivityDate_UID)

# Compare both versions of the data and return any dates with different values
intensity_daily_comp <- activity_sum_days_wide %>%
    mutate(ActivityDate_floored = floor_date(mdy(ActivityDate), unit = "days")) %>%
    mutate(Id_ActivityDate_UID = paste(Id, ActivityDate_floored, sep = "_")) %>%
    with(merge(
        .,
        intensity_daily_sum_wide,
        by = c("Id_ActivityDate_UID"),
        all = TRUE
    )
    ) %>%
    mutate(diff_minutes_sedentary      = minutes_sedentary      - SedentaryMinutes     ) %>%
    mutate(diff_minutes_lightly_active = minutes_lightly_active - LightlyActiveMinutes ) %>%
    mutate(diff_minutes_fairly_active  = minutes_fairly_active  - FairlyActiveMinutes  ) %>%
    mutate(diff_minutes_very_active    = minutes_very_active    - VeryActiveMinutes    ) %>%
    select(
        Id_ActivityDate_UID,
        diff_minutes_sedentary,
        diff_minutes_lightly_active,
        diff_minutes_fairly_active,
        diff_minutes_very_active
    ) %>%
    filter(!(diff_minutes_sedentary == 0 & 
                 diff_minutes_lightly_active == 0 & 
                 diff_minutes_fairly_active == 0 & 
                 diff_minutes_very_active == 0)
    ) %>%
    arrange(Id_ActivityDate_UID)

# For this table, glimpse() shows enough to demonstrate the validity of the method
glimpse(intensity_daily_comp)
rm(intensity_daily_sum_wide, intensity_daily_comp)
```

The approach above appears to work perfectly, with the exception of the "sedentary minutes" calculation, which is consistently higher in my version.

I think its safe to conclude that I got the mapping correct, given that:
a) It's consistently higher by at least 6 hours, which I suspect is caused by my method counting time asleep as "sedentary minutes" - which is not _technically_ wrong, you know - and
b) Reversing the order of the mapping produces entirely wrong results.

That being said, I still don't know if these are the categories FitBits work in, not another naming convention that the data authors came up with, and I don't know if all FitBit models work this way, so let's do something I should have done from the start: read the manuals.

#### Validating "Exercise Intensity" by reading of manuals

Activity trackers like FitBits detect activity intensity partly by measuring the user's heart rate while exercising: a higher heart rate corresponds with a higher degree of exertion. As of April 2016, the three latest FitBit models with heart-rate tracking were:

* FitBit Blaze [(released January 2016)](https://www.youtube.com/watch?v=3k3DNT54NkA)
* FitBit Charge HR [(released January 2015)](https://blog.fitbit.com/charge-hr-and-surge-available-now-plus-new-charge-colors/)
* FitBit Surge [(released January 2015)](https://blog.fitbit.com/charge-hr-and-surge-available-now-plus-new-charge-colors/)

A quick look through the product manuals for each model confirms they all break down user activity into four default heart-rate zones:

|Product|HR Zone 1|HR Zone 2|HR Zone 3|HR Zone 4|
|:------|:----:|:----:|:----:|:----:|
|[FitBit Blaze](https://staticcs.fitbit.com/content/assets/help/manuals/manual_blaze_en_US.pdf)           |"Out of Zone"|"Fat burn"|"Cardio"|"Peak"|
|[FitBit Charge HR](https://staticcs.fitbit.com/content/assets/help/manuals/manual_charge_hr_en_US.pdf)   |"Out of Zone"|"Fat burn"|"Cardio"|"Peak"|
|[FitBit Surge](https://myhelp.fitbit.com/resource/manual_surge_en_US)                                    |"Out of Zone"|"Fat burn"|"Cardio"|"Peak"|

While these aren't exactly the same terms as used in the data set, they're clearly related - "Out of Zone" equates to "Sedentary", for example. 

All three FitBit manuals also make the same claim that the default zones are "based on American Heart Association recommendations". Even without [validating that claim](https://www.heart.org/en/healthy-living/fitness/fitness-basics/aha-recs-for-physical-activity-in-adults), it indicates to me that the reasoning behind each zone is not arbitrary, and is consistent across devices, so I think I can assume any other FitBit models circa 2016 would follow the same classification scheme.

At this point, I'm satisfied that the below are the only four intensity levels I need to consider when analysing the data set, regardless of what models of FitBits were being used:

0. Sedentary Minutes
1. Lightly Active Minutes
2. Fairly Active Minutes
3. Very Active Minutes

#### Validating "Sleep Quality" by use of R _and_ reading of manuals

As with exercise intensity, I start by determining the range of values present in the data:

``` {r find_sleep_min_max}
cat("Min Sleep:", min(sleep_src_mins_tall$value),
    "\nMax Sleep:", max(sleep_src_mins_tall$value), "\n", sep = "")
```

The values for sleep quality range from 1 to 3. The summary data tables for sleep quality introduce only two new variable names: "Total Time In Bed", and "Total Minutes Asleep". There's not a valid name for each level of factor like there was for exercise, so in this case we go straight back to the manuals:

* Blaze: tracks "both your time spent asleep and your sleep quality"
* Charge HR: tracks "the hours you sleep and your movement during the night"
* Surge: tracks "the hours you sleep and your movement during the night"

Further poking around the FitBit help pages on [how to track sleep stats](https://help.fitbit.com/articles/en_US/Help_article/1314.htm) and [what they all mean](https://help.fitbit.com/articles/en_US/Help_article/2163.htm) reveals that different devices track slightly different data if they have heart rate tracking:

* No HR tracking: Generic sleep quality tracking with "Time spent awake, restless, and asleep" categories
* HR tracking: Sleep stage tracking with "Light Sleep, Deep Sleep, and REM Sleep" stages

The help pages also single out the Charge HR and the Surge as the only HR-tracking FitBits to _not_ have full sleep stage tracking, leaving the Blaze as the only device from this time period with that feature. Blaze aside, motion-based sleep quality tracking appears to go all the way back to the [FitBit One](https://myhelp.fitbit.com/s/products?language=en_US&p=one). Given this information, it seems fair to assume the following mapping for the sleep data:

* 1: Awake
* 2: Restless
* 3: Asleep

I can confirm this mapping by attempting to recreare duplicate the summary data in sleepDay_merged:

As with the intensity data, I can confirm this by recreating the data with that naming convention:

```{r compare_sleepDay_data, eval=FALSE}
# Generate my version of sleepDay_merged for comparison with the original
sleep_src_mins_tall_NW <- sleep_src_mins_tall %>%
    # mutate(date_typed = mdy_hms(date)) %>%
    mutate(date_floored = floor_date(mdy_hms(date), unit = "days")) %>%
    # Sum time asleep for each Log ID
    group_by(logId) %>%
    summarize(
        "Id" = min(Id),
        # Associate each Log ID with the latest date recorded under it
        "SleepDay" = max(date_floored),
        "minutes_in_bed"   = n(),
        "minutes_awake"    = sum(case_when(value == 3 ~ 1, TRUE ~ 0)),
        "minutes_restless" = sum(case_when(value == 2 ~ 1, TRUE ~ 0)),
        "minutes_asleep"   = sum(case_when(value == 1 ~ 1, TRUE ~ 0))
    ) %>%
    # Sum time asleep for each date based on SleepDay
    group_by(Id, SleepDay) %>%
    summarize(
        "TotalSleepRecords_2" = n(),
        "TotalMinutesAsleep_2" = sum(minutes_asleep),
        "TotalTimeInBed_2" = sum(minutes_in_bed),
        "TotalMinutesAwake" = sum(minutes_awake),
        "TotalMinutesRestless" = sum(minutes_restless),
    ) %>%
    mutate("Id_SleepDay_UID" = paste(Id, SleepDay, sep = "_")) %>%
    arrange(Id_SleepDay_UID)

# Compare both versions of the data and return any dates with different values
sleepDay_comp <- sleep_sum_days_wide %>%
    # mutate("SleepDay_typed" = mdy_hms(SleepDay)) %>%
    mutate("SleepDay_floored" = floor_date(mdy_hms(SleepDay), unit = "days")) %>%
    mutate("Id_SleepDay_UID" = paste(Id, SleepDay_floored, sep = "_")) %>%
    arrange(Id_SleepDay_UID) %>%
    with(merge(
        .,
        sleep_src_mins_tall_NW,
        by = c("Id_SleepDay_UID"),
        all = TRUE
    )
    ) %>%
    mutate(recordDiff = TotalSleepRecords_2 - TotalSleepRecords) %>%
    mutate(sleepDiff = TotalMinutesAsleep_2 - TotalMinutesAsleep) %>%
    mutate(bedDiff = TotalTimeInBed_2 - TotalTimeInBed) %>%
    select(
        Id_SleepDay_UID,
        recordDiff,
        sleepDiff,
        bedDiff
    ) %>%
    filter(!(recordDiff == 0 & sleepDiff == 0 & bedDiff == 0)) %>%
    arrange(Id_SleepDay_UID)

# For this table, glimpse() shows enough to demonstrate the validity of the method
glimpse(sleepDay_comp)
rm(sleep_src_mins_tall_NW, sleepDay_comp)
```

I was able to recreate the existing sleep_src_mins_tall table almost perfectly by summing sleep times per Log ID, with the latest date associated with each Log ID being used as the "date" for that sleep. In practice it turns out I had the mapping inverted, and actually the following is used:

* 1: Asleep
* 2: Restless
* 3: Awake

So I guess read that as "1 is highest-quality sleep, 3 is worst-quality".

My version of the data contains a few rows that vary slightly from the original, by 1 to 22 minutes. I haven't been able to determine the source of this error, but they're more than close enough to confirm I don't have the factor level mapping backwards, so I'm ready to proceed.

### Duplicate data between tables

The data set includes some tables that contain source data for a given feature, e.g. heart-rate tracking, and others that contain summary data, e.g. everything in the "activity_days_sum_wide" table. 

Some of these are useful, for instance: 

* dailyActivities_merged.csv calculates Sedentary Minutes by excluding time spent asleep. This either requires clever/time-consuming cross-referencing with other tables, or is raw data from a calculation performed on the FitBit itself: either way, I don't want to have to do it again
* sleepDay_merged.csv summarises sleep on a per-night basis, which is more useful for comparing users than the raw, minute-by-minute sleep data

Others are not useful, for instance: 

* calories_sum_mins_wide.csv, intensity_sum_mins_wide.csv, and steps_sum_mins_wide.csv all just pivot minute-level data into one 60-column row per hour containing the same data
* minuteIntensitiesWide_merged.csv just sums and averages intensity per hour

Those tables that do not provide useful summaries can be excluded from the data analysis: if a specific need is found for their data, they can be reloaded or recreated manually as required.

### Duplicate data within tables

The "bodycomp_logs_src_wide.csv" file contains both kilogram and pound variables: these describe the same information, and all of the observations contain values for both variables, so one variable can be dropped with no loss of data. The choice between the two formats seems arbitrary for my analysis, so I'm choosing to keep the kilos data as its expressed in an SI unit.

intensity_sum_hours_wide.csv contains Total Intensity and Average Intensity. Total intensity values exceed 4, the maximum for intensity, and so are not actually useful. Average Intensity is just the Total Intensity for each hour divided by 60 minutes per hour.

# Step 3: Clean
## TODO: Data Cleaning Checklist

* Convert list into changelog format

## Update file names

For this analysis, I'll rename the files to use this naming convention:

> [feature]\_[src/sum]\_[interval]\_[shape].[filetype]

Applying the naming convention to the data set yields the following file names:

| Original                           | Updated                        |
|:-----------------------------------|:-------------------------------|
| dailyActivity_merged.csv           | activity_sum_days_wide.csv     |
| dailyCalories_merged.csv           | calories_sum_days_tall.csv     |
| dailyIntensities_merged.csv        | intensity_sum_days_wide.csv    |
| dailySteps_merged.csv              | steps_sum_days_tall.csv        |
| heartrate_seconds_merged.csv       | heartrate_src_seconds_tall.csv |
| hourlyCalories_merged.csv          | calories_sum_hours_tall.csv    |
| hourlyIntensities_merged.csv       | intensity_sum_hours_wide.csv   |
| hourlySteps_merged.csv             | steps_sum_hours_tall.csv       |
| minuteCaloriesNarrow_merged.csv    | calories_src_mins_tall.csv     |
| minuteCaloriesWide_merged.csv      | calories_sum_mins_wide.csv     |
| minuteIntensitiesNarrow_merged.csv | intensity_src_mins_tall.csv    |
| minuteIntensitiesWide_merged.csv   | intensity_sum_mins_wide.csv    |
| minuteMETsNarrow_merged.csv        | mets_src_mins_tall.csv         |
| minuteSleep_merged.csv             | sleep_src_mins_tall.csv        |
| minuteStepsNarrow_merged.csv       | steps_src_mins_tall.csv        |
| minuteStepsWide_merged.csv         | steps_sum_mins_wide.csv        |
| sleepDay_merged.csv                | sleep_sum_days_wide.csv        |
| weightLogInfo_merged.csv           | bodycomp_src_logs_wide.csv     |

The conversion was performed manually on my local device.

```{r glimpse_data, include=FALSE, eval=FALSE}
# Glimpse all data frames for a quick overview of the data set
# Omitted from report and evaluation: for personal viz purposes only
for (name in df_names) {
    df <- get(name)
    cat("\n", "Dataframe: ", name, "\n", sep = "")
    str(df)
}
```

## Check for missing data

Each data frame was checked for NULL values and non-null empty strings. 

The only table with missing data was "bodycomp_src_logs_wide", which was missing all but two of the data points for Body Fat Percentage. Given this lack of data, Body Fat Percentage will be excluded from the data analysis.

```{r clean_nulls}
# Check for NULLs
cat("Checking for NULL/empty values...\n", sep="")
for (df_name in df_names) {
  df <- get(df_name)
  num_nulls <- sum(is.na(df))
  if(num_nulls) {
    cat(df_name,": ",num_nulls," NULLs","\n",sep="")
  }
  # Check for empty strings (which do not show up as NULLs)
  empty_strings <- df %>%
    filter(if_any(where(is.character), ~ nchar(.) == 0))
  num_empty_strings = nrow(empty_strings)
  if(num_empty_strings) {
    cat(df_name,": ",num_empty_strings," empty strings","\n",sep="")
  }
}
cat("Checking for NULL/empty values done.\n", sep="")
rm(df, df_name, num_nulls, empty_strings, num_empty_strings, df_name)
```


* Looked for NULLs and empty strings. The "bodycomp_src_logs_wide" table was found to be missing the majority of the entries for the body-fat percentage variable: this variable has been excluded from the analysis.

## Clean and update variable names

```{r update_variable_names}

# Function Declarations ----

rename_df_variables <- function(df_name, var_mods) {
  cat("Renaming variables in \"",df_name,"\"...\n", sep = "")
  df <- get(df_name)
  # Check each var name requiring correction against the var names in the df
  for (i in 1:nrow(var_mods)) {
    var_old = var_mods$var_old[i]
    if (!(var_old %in% colnames(df))) {
      next
    }
    # If found, make sure the conversion is applicable to this or all dfs
    tbl <- var_mods$tbl[i]
    if (tbl != "" && tbl != df_name) {
      next
    }
    # Perform the conversion if all checks passed
    var_new = var_mods$var_new[i]
    cat("df: ",df_name, "\tvar_old: ",var_old,"\t",sep="")
    cat("var_new: ",var_new,"\t", sep="")
    cat("tbl: ",tbl,"    ", sep="")
    cat("Replacing... ", sep = "")
    df <- df %>% rename(!!var_new := !!var_old)
    cat("Done.\n", sep = "")
  }
  rm(i)
  cat("Renaming variables in \"",df_name,"\" complete.\n", sep = "")
  return(df)
}

# Global Variable Declarations ----

var_mods <- data.frame(
  var_old = character(0),
  var_new  = character(0),
  type_new = character(0),
  tbl = character(0)
)

# WARNING: Ensure table-specific modifications (tbl != "") are positioned above non-specific modifications with matching var_old/var_new values: only the first modification in the list will be applied to matching variables.
# TODO: Eliminate this issue by modifying code to warn/handle conflicting rows

var_mods <- var_mods %>%
  rbind(.,data.frame(var_old="date",                       var_new="bodycomp_datetime",          type_new="POSIXct", tbl="bodycomp_src_logs_wide")) %>%
  rbind(.,data.frame(var_old="time",                       var_new="heart_rate_second",          type_new="POSIXct", tbl="heartrate_src_seconds_tall")) %>%
  rbind(.,data.frame(var_old="value",                      var_new="heart_rate",                 type_new="",        tbl="heartrate_src_seconds_tall")) %>%
  rbind(.,data.frame(var_old="date",                       var_new="sleep_minute",               type_new="POSIXct", tbl="sleep_src_mins_tall")) %>%
  rbind(.,data.frame(var_old="value",                      var_new="sleep_rank",                 type_new="",        tbl="sleep_src_mins_tall")) %>%
  rbind(.,data.frame(var_old="",                           var_new="activity_hour",              type_new="POSIXct", tbl="")) %>%
  rbind(.,data.frame(var_old="",                           var_new="activity_minute",            type_new="POSIXct", tbl="")) %>%
  rbind(.,data.frame(var_old="",                           var_new="sleep_day",                  type_new="POSIXct", tbl="")) %>%
  rbind(.,data.frame(var_old="",                           var_new="id",                         type_new="character", tbl="")) %>%
  rbind(.,data.frame(var_old="log_id",                     var_new="sleep_log_id",               type_new="character", tbl="sleep_src_mins_tall")) %>%
  rbind(.,data.frame(var_old="log_id",                     var_new="bodycomp_log_id",            type_new="character", tbl="bodycomp_src_logs_wide")) %>%
  rbind(.,data.frame(var_old="activity_date",              var_new="activity_day",               type_new="Date",   tbl="")) %>%
  rbind(.,data.frame(var_old="fairly_active_distance",     var_new="distance_fairly_active",     type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="fairly_active_minutes",      var_new="minutes_fairly_active",      type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="light_active_distance",      var_new="distance_lightly_active",    type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="light_active_minutes",       var_new="minutes_lightly_active",     type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="lightly_active_distance",    var_new="distance_lightly_active",    type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="lightly_active_minutes",     var_new="minutes_lightly_active",     type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="logged_activities_distance", var_new="distance_logged_activities", type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="me_ts",                      var_new="mets",                       type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="moderately_active_distance", var_new="distance_moderately_active", type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="sedentary_active_distance",  var_new="distance_sedentary",         type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="sedentary_distance",         var_new="distance_sedentary",         type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="sedentary_active_minutes",   var_new="minutes_sedentary",          type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="sedentary_minutes",          var_new="minutes_sedentary",          type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="step_total",                 var_new="steps_total",                type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="total_distance",             var_new="distance_total",             type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="total_intensity",            var_new="intensity_total",            type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="total_minutes_asleep",       var_new="minutes_asleep_total",       type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="total_sleep_records",        var_new="sleep_records_total",        type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="total_steps",                var_new="steps_total",                type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="total_time_in_bed",          var_new="minutes_in_bed_total",       type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="tracker_distance",           var_new="distance_tracker",           type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="very_active_distance",       var_new="distance_very_active",       type_new="",        tbl="")) %>%
  rbind(.,data.frame(var_old="very_active_minutes",        var_new="minutes_very_active",        type_new="",        tbl=""))

## Rename Variables ----

cat("Cleaning variable names...\n", sep = "")
for(df_name in df_names) {
  cat("Cleaning ",df_name,"...\n", sep = "")
  assign(df_name, get(df_name) %>% clean_names())
}
cat("Cleaning variable names complete.\n", sep = "")

cat("Renaming variables...\n", sep = "")
var_mods_rename <- var_mods %>%
  filter(var_old != "" & var_new != "")
for(df_name in df_names) {
  assign(df_name, rename_df_variables(df_name, var_mods_rename))
}
rm(df_name)
cat("Renaming variables complete.\n", sep = "")
```

## Remove duplicate data

All dataframes were checked for duplicate rows using the anyDuplicated() function, and duplicate rows were removed using the distinct() function. The function was tested using a prototype version that generated dataframes of all of the apparent duplicate rows: these were verified manually before the function was allowed to modify the actual dataframes. The logic was verified again by re-running it after the initial removal: all dataframes reported zero duplicates on the second run, confirming the success of the first pass.

```{r remove_duplicates}
cat("Checking for duplicate values...\n",sep="")
for (df_name in df_names) {
  cat("Checking ",df_name," for duplicates... ",sep="")
  df <- get(df_name)
  if (!anyDuplicated(df)) {
    cat("0 duplicates removed. Done.\n",sep="")
  } else {
    nrow_before <- nrow(df)
    df <- distinct(df)
    nrow_after <- nrow(df)
    cat("Removing ",(nrow_before - nrow_after)," duplicates... ",sep="")
    assign(df_name, df)
    cat("Done.\n",sep="")
  }
}
rm(df, df_name, nrow_before, nrow_after)
cat("Checking for duplicate values complete.\n",sep="")
```

## Recast mismatched variable data types

| Variable        | Original Type | Updated Type | Reason                                                           |
|:----------------|:--------------|:-------------|:-----------------------------------------------------------------|
| activity_day    | chr           | datetime     | Cannot perform datetime operations on chr variables              |
| activity_hour   | chr           | datetime     | Cannot perform datetime operations on chr variables              |
| activity_minute | chr           | datetime     | Cannot perform datetime operations on chr variables              |
| date            | chr           | datetime     | Cannot perform datetime operations on chr variables              |
| id              | num           | chr          | Disable numerical operations (IDs are considered a UID string)   |
| log_id          | num           | chr          | Disable numerical operations (IDs are considered a UID string)   |
| time            | chr           | datetime     | Cannot perform datetime operations on chr variables              |
| sleep_rank      | num           | factor 1:3   | Disable numerical operations (value is a ranking, not an amount) |
| intensity       | num           | factor 0:3   | Disable numerical operations (value is a ranking, not an amount) |


```{r update_variable_types_nonfactors}

# Global Variable Declarations ----

var_mods_recast <- var_mods %>%
  filter(type_new != "")

# Function Declarations ----

are_identical_lists <- function(list1, list2) {
  if (length(list1) != length(list2)) {
    return(FALSE)
  }
  for (i in seq_along(list1)) {
    if(!identical(list1[[i]], list2[[i]])) {
      cat("Non-identical lists at list1[",i,"]. Exiting.\n", sep = "")
      print(list1[[i]])
      print(list2[[i]])
      rm(i)
      return(FALSE)
    }
  }
  rm(i)
  return(TRUE)
}

get_df_var_types <- function(df_name) {
  cat("Getting current variable types for ", df_name, "... ", sep = "")
  df <- get(df_name)
  var_types <- data.frame(
    var = names(df),
    type = sapply(df, function(col) class(col)[1])
  )
  cat("Done.\n", sep = "")
  return(var_types)
}

get_df_target_var_types <- function(df_name) {
  cat("Getting target variable types for ", df_name, "...\n", sep = "")
  var_types <- get_df_var_types(df_name)
  # Iterate over rows in current var_types
  for (var_row in 1:nrow(var_types)) {
    # Check if var name is in var_mods_recast
    var_name <- var_types$var[var_row]
    for(mods_row in 1:nrow(var_mods_recast)) {
      var_name_mods <- var_mods_recast$var_new[mods_row]
      # If no, skip: if yes, replace type with target
      if(var_name == var_name_mods) {
        type_new <- var_mods_recast$type_new[mods_row]
        cat("Updated \"", var_name, "\" from ", var_types$type[var_row], " to ", var_mods_recast$type_new[mods_row], ".\n", sep = "")
        var_types$type[var_row] <- type_new
      }
    }
  }
  cat("Getting target variable types for ", df_name, " done.\n", sep = "")
  return(var_types)
}

recast_variables <- function(df_name) {
  df <- get(df_name)
  for (i in 1:nrow(var_mods_recast)) {
    var_new <- var_mods_recast$var_new[i]
    type_new <- var_mods_recast$type_new[i]
    if (var_new %in% colnames(df)) {
      cat("Converting ",df_name,"$",var_new," to ",type_new, "... ", sep = "")
      if (type_new == "character") {
        df <- df %>% mutate("{var_new}" := as.character(!!sym(var_new)))
      } else if (type_new == "Date") {
        df <- df %>% mutate("{var_new}" := mdy(!!sym(var_new)))
      } else if (type_new == "POSIXct") {
        df <- df %>% mutate("{var_new}" := mdy_hms(!!sym(var_new)))
      } else {
        cat("type_new not found: not converting.", sep = "")
      }
      cat("Done.\n", sep = "")
    }
  }
  rm(i)
  return(df)
}

# Recast Variables ----

cat("Generating list of target column types for testing...\n", sep = "")
df_types_target <-lapply(df_names, get_df_target_var_types)
names(df_types_target) <- df_names
cat("Generating list of target column types complete.\n", sep = "")

cat("Recasting variables...\n", sep = "")
for (df_name in df_names) {
  cat("Recasting ",df_name,"...\n", sep = "")
  assign(df_name, recast_variables(df_name))
  cat("Converting ",df_name," complete.\n", sep = "")
}
cat("Recasting variables complete.\n", sep = "")

# Test Recasting of Variables ----

cat("Generating list of updated column types...\n", sep = "")
df_types_after <- lapply(df_names, get_df_var_types)
names(df_types_after) <- df_names
cat("Generating list of updated column types complete.\n", sep = "")

cat("Checking updated column types against target types...\n", sep = "")
test_succeeded <- are_identical_lists(df_types_after, df_types_target)
cat("Checking updated column types against target types complete.\n", sep = "")
cat("Data recasting ", case_when(test_succeeded ~ "succeeded", TRUE ~"failed"), ".", sep = "")
rm(df_name, df_types_target, df_types_after, test_succeeded)
```

``` {r update_variable_types_factors}
# Individual recast of variables with only one occurrence
sleep_src_mins_tall <- sleep_src_mins_tall %>%
    mutate(sleep_rank = factor(sleep_rank, levels = 1:3, labels = c("Asleep", "Restless", "Awake")))

intensity_src_mins_tall <- intensity_src_mins_tall %>%
    mutate(intensity = factor(intensity, levels = 0:3, labels = c("Sedentary", "Lightly Active", "Fairly Active", "Very Active")))
```

Given the large number of variables in the data set, the recasting procedure includes test code to confirm the updated variable types match the types specified in the variable mods list. The test code works by first generating a list of the desired final variable/type pairs, then, once the conversion is completed, generating a second list of the actual variable/type pairs in the data frames to compare it to. This has the advantage of not just confirming the desired conversions took place, but also checks for any unintended changes to variables that did not require conversion.

Building the test code added a decent amount of work to the project, but now I have it working, it can be reused and scaled to future projects.

## Trim leading or trailing characters

* Manual check for variable names with non-whitespace trailing characters
* Automated trim of variable names
* Automated trim of character-type data entries

```{r trim_whitespace_variables}
cat("Trimming whitespace in variable names...\n", sep="")
for (df_name in df_names) {
  df <- get(df_name)
  for (col_name in colnames(df)) {
    col_name_trimmed <- str_trim(col_name)
    cat("Trimming ",df_name, "[",col_name,"] to ",col_name_trimmed,"... ",sep="")
    df <- df %>% rename(!!col_name := !!col_name_trimmed)
    cat("Done.\n", sep = "")
  }
}
rm(df, df_name, col_name, col_name_trimmed)
cat("Trimming whitespace in variable names complete.\n", sep="")
```

```{r trim_whitespace_data}
trim_chr_column <- function(col) {
  if (is.character(col)) {
    return(str_trim(col))
  } else {
    return(col)
  }
}

cat("Trimming whitespace in character-type data values...\n", sep="")
for (df_name in df_names) {
  df <- get(df_name)
  for (col_name in colnames(df)) {
    if (is.character(df[[col_name]])) {
      cat("Trimming ",df_name, "[",col_name,"]... ",sep="")
      # df <- df %>% mutate(if_any(where(is.character), trim_chr_column))
      df <- df %>% mutate(across(all_of(col_name), str_trim))
      cat("Done.\n", sep = "")
    }
  }
}
rm(df, df_name, col_name)
cat("Trimming whitespace in character-type data values complete.\n", sep="")
```
## Validate numeric data

Given that the data was not entered manually by the user, there's no way for me to manually check the correctness of all of the numeric values included in the data set. The values were instead checked against pre-determined limits to verify that they fall within realistic ranges, as detailed below.

Validating the data in this way also helps confirm the data makes sense in terms of the business logic, by confirming the data falls within realistic ranges given the capabilities of the devices and the types of data they claim to track.

### ID lengths and value ranges are correct

I wrote a short function to validate the length of all rows in a data frame for a given column number and valid length. This was then used to check ID values in the data set against their correct length, including:

* "id" values: 10-digit
* "sleep_log_id" values: 11-digit
* "bodycomp_log_id" values: 13-digit

The validation confirmed all values were the correct length.

```{r validate_ID_lengths}
validate_string_length <- function(df_name, col_name, valid_length) {
  cat("Checking ",df_name," for invalid ",col_name," values... ",sep="")
  df <- get(df_name)
  if (col_name %in% colnames(df)) {
    invalid_values <- df %>% select(!!sym(col_name)) %>% filter(nchar(!!sym(col_name)) != valid_length)
  }
  if (exists('invalid_values') && nrow(invalid_values) > 0) {
    cat(nrow(invalid_values)," invalid values found:\n",sep="")
    glimpse(invalid_values)
    rm(invalid_values)
  } else {
    cat("complete.\n",sep="")
  }
  rm(df)
}

result <- map(df_names, ~validate_string_length(.x, col_name = "id", valid_length = 10))
result <- map(df_names, ~validate_string_length(.x, col_name = "sleep_log_id", valid_length = 11))
result <- map(df_names, ~validate_string_length(.x, col_name = "bodycomp_log_id", valid_length = 13))
rm(result)
```

To further validate the large number of ID data points in the data set, I wrote a short function to check all ID variables in each data frame and determine the minimum and maximum values. The intent was to identify any possible erroneous values within the acceptable length limits, e.g. all zeroes or all nines. The function makes use of the direct conversion of strings to numerals in the min() and max() functions to carry out the comparison on character-type values.

The validation confirmed all values were within a realistic range.

```{r validate_ID_ranges}
cat("Checking min/max ID value ranges...\n",sep="")
id_col_names <- c("id", "sleep_log_id", "bodycomp_log_id")
for (df_name in df_names) {
  df <- get(df_name)
  for (col_name in id_col_names) {
    if(col_name %in% colnames(df)) {
      cat(df_name,"[",col_name,"] Min = ",min(df[[col_name]])," Max = ",max(df[[col_name]]),".\n",sep="")
    }
  }
}
rm(df, df_name, col_name, id_col_names)
cat("Checking min/max ID value ranges complete.\n",sep="")
```

### Dates are all within range

The data set is described as containing data from users collected between "03.12.2016-05.12.2016": all records in the data set were validated against this date range.

```{r validate_all_dates}

validate_dates <- function(df_name) {
  valid_dates_stt <- as.Date("2016-03-12", format = "%Y-%m-%d")
  valid_dates_end <- as.Date("2016-05-14", format = "%Y-%m-%d")
  
  cat("Validating dates for ",df_name,"... ",sep="")
  
  date_columns <- get(df_name) %>%
    select_if(function(col) is.POSIXct(col) || is.Date(col))
  
  if (ncol(date_columns) == 0) {
    cat("Error: no date column found.\n",sep="")
  } else if (ncol(date_columns) > 1) {
    cat("Error: more than one date column found for ",df_name,".\n",sep="")
  } else {
    outside_range <- date_columns %>%
      filter(if_any(everything(), ~ . < valid_dates_stt | . > valid_dates_end))
    if (nrow(outside_range) > 0) {
      cat("found ",nrow(outside_range)," invalid values:\n",sep="")
      print(outside_range)
      rm(outside_range)
    } else {
      cat("complete.\n")
    }
  }
  rm(date_columns)
}

result <- lapply(df_names, validate_dates)
rm(result)
```

This check found dates just outside the range, dating up to 8am on 05.13.2016, the day after the data set supposedly ended. I didn't consider this to be a problem, so the valid end-date was updated to the 14th of May 2016 accordingly, and all dates passed this check.

### Non-date values are within appropriate ranges for their units
Checklist:
  - All numeric values are non-negative
  - Percentage values are less than 100
  - Weight values are positive and make sense (e.g. less than 200kg)
  - BMI values are in range
  - Daily, hourly, and minute duration sums are no more than one day, hour, or minute, respectively
  - Distances make sense
  - Step counts make sense (check for >20000 for a start)
  - Calories are within normal range
  - Heart rates are less than 200
  
First, a check for negative values was run on all numeric columns in the data set.

```{r validate_all_numeric}
validate_numerics <- function(df_name) {
  # This function performs all checks on numeric values that are required in more than one data-frame, e.g. non-negativity and summation
  cat("Validating numerics for ",df_name,"... ",sep="")
  numerics <- get(df_name) %>% select_if(is.numeric)
  if (ncol(numerics) == 0) {
    cat("No numeric variables found.\n",sep="")
  } else {
    # Check for negative values
    negative_values <- numerics %>%
      filter(if_any(everything(), ~ . < 0))
    if (nrow(negative_values) > 0) {
      cat("found ",nrow(negative_values)," invalid values:\n",sep="")
      print(negative_values)
    } else {
      cat("complete.\n")
    }
    rm(negative_values)
    # Check for summation
  }
  rm(numerics)
}

result <- lapply(df_names, validate_numerics)
rm(result)
```

Results:

* No negative values were found in the data.

Second, variable-specific checks were run to confirm the data fell within realistic ranges given my understanding of the variables being measured.

Note: The maximum values given are used as thresholds above which values may not be realistic, not as hard limits for acceptability: a heart-rate of 200bpm, for example, is entirely possible, but it is high enough that I would want to check if the data point corresponded to a period of high-intensity exercise.

```{r validate_specific_numeric}
# For a given list of dfs, column names, and a min-max range, check all matching columns in all matching dfs against that range
validate_within_range <- function(df_name, column_names, range_min, range_max) {
  df <- get(df_name)
  for (col_name in column_names) {
    cat("Checking ranges for ",df_name,"[",col_name,"]... ",sep="")
    if (!(col_name %in% colnames(df))) {
      cat("column not found.\n")
    } else {
      out_of_range <- df %>%
        filter(!!sym(col_name) < range_min | !!sym(col_name) > range_max)
      if (nrow(out_of_range) <= 0) {
        cat("complete.\n",sep="")
      } else {
        cat("Found ",nrow(out_of_range)," out-of-range values:\n",sep="")
        glimpse(out_of_range)
      }
      rm(out_of_range)
    }
  }
rm(df, col_name)
}

# Minute summation
valid_df_names <- c(
  "activity_sum_days_wide",
  "intensity_sum_days_wide",
  "sleep_sum_days_wide")
valid_col_names <- c(
  "minutes_very_active",
  "minutes_fairly_active",
  "minutes_lightly_active",
  "minutes_sedentary",
  "minutes_asleep_total",
  "minutes_in_bed_total")
range_max <- 60 * 12 # Minutes in half a day
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# Weights (kg)
valid_df_names <- c("bodycomp_src_logs_wide")
valid_col_names <- c("weight_kg")
range_max <- 150 # Arbitrarily chosen as high enough to be a potentially erroneous value
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# Weights (pounds, same limit as for weight in kilos)
valid_df_names <- c("bodycomp_src_logs_wide")
valid_col_names <- c("weight_pounds")
kg2lb <- 2.204623
range_max <- 150 * kg2lb # Arbitrarily chosen as high enough to be a potentially erroneous value
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))
rm(kg2lb)

# BMI 
valid_df_names <- c("bodycomp_src_logs_wide")
valid_col_names <- c("bmi")
range_max <- 40.0 # Corresponds with the WHO "Obese (Class III)" weight category
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# Distances
valid_df_names <- c(
  "activity_sum_days_wide",
  "intensity_sum_days_wide")
valid_col_names <- c(
  "distance_lightly_active",
  "distance_logged_activities",
  "distance_moderately_active",
  "distance_sedentary",
  "distance_total",
  "distance_tracker",
  "distance_very_active")
range_max <- 21.08241 # Equivalent to one half-marathon
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# Step counts
valid_df_names <- c(
  "activity_sum_days_wide",
  "steps_sum_days_tall",
  "steps_sum_hours_tall",
  "steps_src_mins_tall")
valid_col_names <- c(
  "steps",
  "steps_total")
range_max <- 14800 # Set to double the average daily step count for Australian adults
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# Calories
valid_df_names <- c(
  "activity_sum_days_wide",
  "calories_src_mins_tall",
  "calories_sum_days_tall",
  "calories_sum_hours_tall")
valid_col_names <- c("calories")
range_max <- 4000 # Chosen arbitrarily as double the typically-recommended daily caloric intake
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# TODO: METs
valid_df_names <- c("mets_src_mins_tall")
valid_col_names <- c("mets")
range_max <- 12 # Equivalent to vigourous squash playing
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

# Heart Rates
valid_df_names <- c("heartrate_src_seconds_tall")
valid_col_names <- c("heart_rate")
range_max  <- 200 # Chosen based on average 100% heart-rate for a 20 y.o. (Source: American Heart Foundation)
result <- map(valid_df_names, ~validate_within_range(.x, valid_col_names, 0, range_max))

rm(range_max, result, valid_df_names, valid_col_names)
```

Results:
  
  * There were no invalid negative values
  * There were no invalid percentage values
  * There were no invalid minute summations
  * There were 19 step count data points above 20,000: these were manually checked and found to be associated with long distances covered and high levels of exercise, which made sense, so I considered that data valid.
  * There were a total of four unique data points with a distance covered of more than one half-marathon: all of these also indicatd a very high level of exercise for at lest 90 minutes, which made sense, so I considered the data valid
  * Calories: 21 records showed caloric burns of over 4000 calories. All but one of those were associated with very high levels of activity: the outlier showed only 30 minutes total as "Very Active" or "Lightly Active", compared to 120+ minutes for all other records. It does show 15km distance covered that day, which is nearly a third of a marathon, so for now it makes enough sense to keep it in, and I will analyse it further after the cleaning stage.
  * Heart-rate: 13 records were found with a heart-rate between 200 and 203, all of which corresponded to a single user over a 30-minute period of "Very Active" intensity.
  * METs: Every value was out of range for the original limit of 12 METs, chosen as it was the highest listed MET value for any activity in the source data. The smallest repeated value in the data was 10. This suggests my interpretation of the data logged for this variable is wrong.
  
#### Validating the METs data

* In theory, METs are a rate of energy expenditure: if you expend energy at a rate of 3 METs, that value doesn't change whether you maintain it for a minute or an hour
* The highest MET value for any given activity in the sources I used was 12, for sustained "heavy" squash playing
* My initial assumption was that the MET values in the data were the average MET rate for the sampling period, in this case one minute
* The data points, however, were listing MET values as high as 157
* This suggested the values were not averages but rather some sum or projected sum, e.g. projected MET-hours based on the average over the minute.
* Searching through the product manuals again revealed no information on METs at all. A [Help article](https://help.fitbit.com/articles/en_US/Help_article/1379.htm) was the only other official reference to METs that I could find: it implies that METs are used to calculate "Active Minutes", and that Active Minutes count double in higher heart-rate zones, but no specific mathematical relationship between the two was described.

With this in mind, I attempted to corroborate the METs data by plotting it against other related data, specifically calories-burned and heart-rate. Individual charts were plotted for each user ID.

```{r interpret_MET_data, eval=FALSE}
# Generate minute-level summary heart-rate data for merging with calorie/
heartrate_sum_minutes_tall <- heartrate_src_seconds_tall %>% 
  mutate(activity_minute = floor_date(heart_rate_second, unit = "minute")) %>%
  group_by(id, activity_minute) %>%
  summarize(heart_rate_mean = mean(heart_rate))

mets_vs_heart_rate <- mets_src_mins_tall %>%
  merge(., intensity_src_mins_tall, by = c("id", "activity_minute")) %>%
  merge(., heartrate_sum_minutes_tall, by = c("id", "activity_minute"))

mets_vs_calories <- mets_src_mins_tall %>%
  merge(., intensity_src_mins_tall, by = c("id", "activity_minute")) %>%
  merge(., calories_src_mins_tall, by = c("id", "activity_minute"))

plot_mets_vs_heartrate <- ggplot(data = mets_vs_heart_rate) +
  geom_point(mapping=aes(x=heart_rate_mean, y=mets, shape=intensity, color=intensity)) +
  geom_smooth(mapping=aes(x=heart_rate_mean, y=mets), method = "lm", se = FALSE, color = "blue") +
  stat_cor(aes(x=heart_rate_mean, y=mets, label=..rr.label..), label.x=0, label.y=50) +
  facet_wrap(vars(id))
print(plot_mets_vs_heartrate)

plot_mets_vs_calories <- ggplot(data = mets_vs_calories) +
  geom_point(mapping=aes(x=calories,y=mets, shape=intensity, color=intensity)) +
  # geom_smooth(mapping=aes(x=calories,y=mets), method = "lm", se = FALSE, color = "blue") +
  stat_cor(aes(x=calories, y=mets, label=..rr.label..), label.x=0, label.y=50) +
  facet_wrap(vars(id))
print(plot_mets_vs_calories)

plot_mets_vs_intensity <- ggplot(data = mets_vs_calories) +
  geom_point(mapping=aes(x=intensity,y=mets)) +
  # geom_smooth(mapping=aes(x=calories,y=mets), method = "lm", se = FALSE, color = "blue") +
  #stat_cor(aes(x=calories, y=mets, label=..rr.label..), label.x=0, label.y=50) +
  facet_wrap(vars(id))
print(plot_mets_vs_intensity)

```

Results:

* There was a clear positive correlation between heart-rate and METs, with R^2 values ranging from 0.43 to 0.82
* A much tighter positive correlation was present between calories and METs, with R^2 values of 1 across all user ID values.

Given the very tight relationship between METs and calories, and the unclear sampling method used to generate the MET data, I decided to keep the METs data as-is, and consider all values as valid for the purposes of cleaning..After cleaning the data, further analysis of the MET data may shed more light on its meaning: in the meantime, the calories, heart-rate, and intensity variables can be used to analyse energy expenditure and activity type.
  
#### Sources used to determine realistic limits for numerical values
Sources for ranges:
  * BMI: Based on WHO statistical categories for classifying BMI values: https://en.wikipedia.org/wiki/Body_mass_index#Categories
  * Half-marathon: https://en.wikipedia.org/wiki/Half_marathon
  * Step-count: https://www.abs.gov.au/statistics/health/health-conditions-and-risks/australian-health-survey-physical-activity/latest-release#pedometer-steps
  * Caloric intake: Chosen arbitrarily as double my own daily required intake, based on a calculator accessed at: https://www.eatforhealth.gov.au/nutrition-calculators/daily-energy-requirements-calculator 
  * Max HR: https://www.heart.org/en/healthy-living/fitness/fitness-basics/target-heart-rates
  * METs: https://www.hsph.harvard.edu/nutritionsource/staying-active/, https://onlinelibrary.wiley.com/doi/epdf/10.1002/clc.4960130809


# TODO Step 4: Analyse
In which I do my actual analysis

## Analysing function usage

### Method

This part of the analysis uses the unique IDs to determine what proportion of the sample base uses each feature. This works two ways: I located which IDs are in the data for each feature, and I located which features are in the data for each ID.

```{r list_of_all_dataframes, eval=FALSE}
dfs <- mget(df_names)
for (i in seq_along(data)) {
  print(names(data)[i])
  glimpse(data[[i]])
}
```

```{r unique_ids_per_feature}
feature_usage = data.frame(
  feature = character(0),
  df_name = character(0)
) %>%
  rbind(.,data.frame(feature="Heart Rate",         df_name="heartrate_src_seconds_tall")) %>%
  rbind(.,data.frame(feature="Calorie Burn",       df_name="calories_src_mins_tall")) %>%
  rbind(.,data.frame(feature="Exercise Intensity", df_name="intensity_src_mins_tall")) %>%
  rbind(.,data.frame(feature="MET Burn",           df_name="mets_src_mins_tall")) %>%
  rbind(.,data.frame(feature="Step Count",         df_name="steps_src_mins_tall")) %>%
  rbind(.,data.frame(feature="Sleep Tracking",     df_name="sleep_src_mins_tall")) %>%
  rbind(.,data.frame(feature="Body Composition",   df_name="bodycomp_src_logs_wide"))

unique_ids <- data.frame(
  df_name = character(0),
  id_count = numeric(0),
  id_list = I(list())
)

for (i in 1:nrow(feature_usage)) {
  name <- feature_usage$df_name[i]
  df <- get(name)
  if (!("id" %in% colnames(df))) {
    cat("Warning: ",name,"[id] not found.\n",sep="")
  } else {
    id_vect <- unique(df$id)
    id_count = length(id_vect)
    unique_ids <- unique_ids %>%
      rbind(.,data.frame(
        df_name=name,
        id_count=id_count,
        id_list=I(list(id_vect))))
  }
}
rm(df, i, id_count, id_vect, name)

feature_usage <- merge(feature_usage, unique_ids, by="df_name", all=TRUE)
feature_usage <- feature_usage %>%
  arrange(desc(id_count))
print(feature_usage)
rm(unique_ids)
```

### Total Feature Usage Findings

The first analysis is a simple record of how many users are logging data for each function

```{r, viz_unique_IDs_by_feature}
max_ids <- 33 # From manual inspection of the data

custom_order <- feature_usage$feature
feature_usage_sorted <- feature_usage
feature_usage_sorted$feature <- factor(feature_usage_sorted$feature, levels = custom_order)

ggplot(feature_usage_sorted, aes(feature, id_count)) +
  geom_col() +
  coord_cartesian(ylim=c(0L, max_ids)) +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  labs(x="Feature", y="Unique ID Count", title="Unique ID Counts for Fitabase Features")
rm(custom_order, feature_usage_sorted)
```

Findings on feature popularity:

1. Calorie Burn, Exercise Intensity, MET Burn, and Step Count, with 100% of users
2. Sleep tracking, with 73% of users
3. Heart Rate tracking, with 42% of users
4. Body Composition logging, with 24% of users

### Unique ID Feature Usage Findings

In this section, I wanted to plot which IDs were using which features. I decided to go with an X-Y scatter plot with ID on the X-axis and feature name on the Y-axis, to create a table

```{r, count_features_by_unique_ID}
# Identify all unique IDs in database
unique_ids <- vector(mode="character")
for (df_name in df_names) {
  df <- get(df_name)
  if ("id" %in% colnames(df)) {
    unique_ids <- unique(c(unique_ids, unique(df$id)))
  }
}

feature_tibble_long <- tibble() %>%
  add_column(id := character(0)) %>%
  add_column(feature := character(0)) %>%
  add_column(used := logical(0))

for (unique_id in unique_ids) {
  for (j in 1:nrow(feature_usage)) {
    feature_name <- feature_usage$feature[j]
    id_list <- feature_usage$id_list[[j]]
    feature_used <- FALSE
    if (unique_id %in% id_list) {
      cat("Found id \"",unique_id,"\" using ",feature_name,".\n",sep="")
      feature_used <- TRUE
    } else {
      cat("Did not find id \"",unique_id,"\" using ",feature_name,".\n",sep="")
    }
    feature_tibble_long <- feature_tibble_long %>%
      add_row(id=unique_id,feature=feature_name,used=feature_used)
  }
}

feature_usage_summary <- feature_tibble_long %>% 
  group_by(feature) %>%
  summarize(sum_used = sum(used, na.rm = TRUE)) %>%
  arrange(desc(sum_used))

id_usage_summary <- feature_tibble_long %>% 
  group_by(id) %>%
  summarize(sum_used = sum(used, na.rm = TRUE))
```

``` {r viz_feature_usage}

custom_order <- feature_usage_summary$feature
feature_usage_sorted <- feature_tibble_long
feature_usage_sorted$feature <- factor(feature_usage_sorted$feature, levels = custom_order)


ggplot(feature_usage_sorted, aes(x = feature, y = id, color = used)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("TRUE" = "blue", "FALSE" = "red")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Feature Usage by ID",
       x = "Feature",
       y = "ID",
       color = "Feature Used")
```

### Note on different FitBit devices

The dataset does not indicate what type of FitBit each participant was using: it is possible the less-used features are unavailable on certain devices. This means the lack of usage may not be due to a lack of interest: users may be buying less-capable devices due to cost considerations, for example


## Analysing the most-used features

In order to develop a targeted marketing strategy for the Ivy, 

Analyses for each feature:
* Are people using these devices athletes trying to get an edge, or slackers tracking their sedentary lifestyles?
* Does anything change over the month people use their device? In other words, can we claim the usage is correlated to improvements?

### TO OMIT: Calorie and MET Tracking

Firstly I have a look at the overall daily calorie burn for our users:

```{r average_daily_calories}
mean_daily_cals <- calories_sum_days_tall %>%
  group_by(id) %>%
  summarize("mean_calories" = mean(calories))

ggplot(mean_daily_cals, aes(x=reorder(id, mean_calories), y=mean_calories)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

I found a fairly even spread of uses: using 2000 calories as a benchmark value, most IDs are at or above that benchmark. Lets get a more explicit breakdown by building a histogram, binning IDs into 250-calorie groups:

```{r mean_daily_cals_histogram}
ggplot(mean_daily_cals, aes(x=mean_calories)) +
  geom_histogram(binwidth=250)
```

Here we get a clearer view of the distribution: the largest cohort is in the 2000-calorie range, with a small number below that, and a long tail increasing up to 3500 calories per day on average. An implication here is that the majority of users are engaging in exercise, but calories burned alone don't confirm that to my satisfaction.

At this point, I would ideally cross-reference the calorie data with body composition data to see if the higher-calorie IDs are associated with users with a higher BMI or weight, which would imply a higher base energy requirement. However, only 8 of the 33 users have logged body composition, versus all 33 users who logged calories, so this would not be comprehensive.

I can use the Exercise Intensity metric to get a better idea of who's burning more calories. To get a baseline view, I'll visualise the average time spent in each of the four Intensity zones for each user:

```{r viz_average_daily_intensities_by_id}
# For each ID, generate averages for the three non-sedentary intensity levels

mean_daily_intensities_wide <- intensity_sum_days_wide %>%
  group_by(id) %>%
  summarize("sedentary" = mean(minutes_sedentary),
            "lightly_active" = mean(minutes_lightly_active),
            "fairly_active" = mean(minutes_fairly_active),
            "very_active" = mean(minutes_very_active))

#Convert data to long-format for plotting as a histogram
intensity_order = c("sedentary", "lightly_active", "fairly_active", "very_active")
mean_daily_intensities <- mean_daily_intensities_wide %>%
  tidyr::gather(key = "intensity", value = "mean_minutes", -id) %>%
  mutate(intensity = factor(intensity, levels = intensity_order))

# Generate data to order IDs by average "Very Active" time
mean_very_active_time_by_id <- mean_daily_intensities %>%
  filter(intensity == "very_active") %>%
  arrange(mean_minutes)

#Convert ID to factor ordered by average "Very Active" time
mean_daily_intensities$id <- factor(
  mean_daily_intensities$id,
  levels = mean_very_active_time_by_id$id)

# Comment out this line to include Sedentary time in the graph
#mean_daily_intensities <- mean_daily_intensities %>% filter(intensity != "sedentary")

ggplot(mean_daily_intensities, aes(x= id, y= mean_minutes, fill= intensity)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Average Time by Intensity Zone",
       x = "User ID",
       y = "Total Average Daily Time (minutes)")
```

Immediately, a trend is clear:
* The majority of users' time is spent "Sedentary"
* The majority of non-Sedentary time is spent "Lightly Active"

Caveats:
* The averages don't add up to full weeks, so its possible people aren't tracking all of their exercising.

The original idea here was to see who's burning the calories and how, so now let's plot active time against average calories burned. For this I generated three graphs, plotting calories burned against:
* Average non-sedentary time (sum of Lightly, Fairly, and Very Active minutes)
* Average active time (sum of Fairly and Very Active minutes)
* Average very-active time (sum of Very Active minutes)

```{r calories_vs_activity}
mean_non_sedentary_time <- intensity_sum_days_wide %>%
  group_by(id, activity_day) %>%
  summarize("non_sedentary_time" = sum(minutes_lightly_active,
                                       minutes_fairly_active,
                                       minutes_very_active)) %>%
  group_by(id) %>%
  summarize("mean_non_sedentary_time" = mean(non_sedentary_time))

cals_vs_activity <- merge(mean_daily_cals, mean_non_sedentary_time, by="id")

plot_cal_vs_non_sedentary <- ggplot(cals_vs_activity) +
  geom_point(aes(x=mean_non_sedentary_time, y=mean_calories)) +
  geom_smooth(mapping=aes(x=mean_non_sedentary_time,y=mean_calories),
              method = "lm",
              se = FALSE,
              color = "blue") +
  stat_cor(aes(x=mean_non_sedentary_time, y=mean_calories, label=..rr.label..),
           label.x=0,
           label.y=0)
```

```{r cals_vs_more_active}
mean_fairly_active_time <- intensity_sum_days_wide %>%
  group_by(id, activity_day) %>%
  summarize("active_time" = sum(minutes_fairly_active,
                                minutes_very_active)) %>%
  group_by(id) %>%
  summarize("mean_fairly_active_time" = mean(active_time))

cals_vs_activity <- merge(mean_daily_cals, mean_fairly_active_time, by="id")

plot_cal_vs_active <- ggplot(cals_vs_activity) +
  geom_point(aes(x=mean_fairly_active_time, y=mean_calories)) +
  geom_smooth(mapping=aes(x=mean_fairly_active_time,y=mean_calories),
              method = "lm",
              se = FALSE,
              color = "blue") +
  stat_cor(aes(x=mean_fairly_active_time, y=mean_calories, label=..rr.label..),
           label.x=0,
           label.y=0)
```

```{r cals_vs_very_active}
mean_very_active_time <- intensity_sum_days_wide %>%
  group_by(id) %>%
  summarize("mean_active_time" = mean(minutes_very_active))

cals_vs_very_active <- merge(mean_daily_cals, mean_very_active_time, by="id")

plot_cal_vs_very_active <- ggplot(cals_vs_very_active, aes(x=mean_active_time, y=mean_calories)) +
  geom_point() +
  geom_smooth(mapping=aes(x=mean_active_time,y=mean_calories), method = "lm", se = FALSE, color = "blue") +
  stat_cor(aes(x=mean_active_time, y=mean_calories, label=..rr.label..), label.x=0, label.y=0)
```

All three plots are shown together for comparison.

```{r plot_cals_vs_intensity}
plot_cal_vs_non_sedentary <- plot_cal_vs_non_sedentary + scale_x_continuous(limits=c(0,350))
plot_cal_vs_active <- plot_cal_vs_active + scale_x_continuous(limits=c(0,350))
plot_cal_vs_very_active <- plot_cal_vs_very_active + scale_x_continuous(limits=c(0,350))

grid.arrange(plot_cal_vs_non_sedentary, plot_cal_vs_active, plot_cal_vs_very_active, nrow = 3)
```

Final Findings:

* Visual inspection of the charts confirms a fairly-obvious theory: more time spent doing higher-intensity activities correlates with a rapid increase in calories burned. The relationship 
* Calories burned is shown to correlate only loosely with total non-sedentary time, with R^2 = 0.041, but more tightly with active and very-active time, with R^2 = 0.32 and R^2 = 0.4, respectively.
* The scatterplot for Very Active time show a small group of outliers with very high calorie burn and Very Active time
* There's a decent spread of calories-burned even at the lower ranges of active time, which is what you'd expect from a group of mostly-sedentary people of presumably different ages, heights, weights, and genders




#### TO OMIT: Workout-based Analysis

A limitation of analysing the overall weekly time spent in the different zones is that it does not identify discrete "sessions" of physical activity. This means that we could get misleadingly similar final numbers for two different lifestyles: for example, one user might do regular short bursts of exertion (such as climbing stairs in an office) but never go to the gym, while a second user might regularly go to the gym but is otherwise sedentary.

To get around this, I took the minute-by-minute intensity data and grouped it not just by ID but by timestamp difference: any timestamps within a certain distance apart were grouped into one "session". The resultant sessions gave me a new data point to analyse.

```{r get_workouts, eval=FALSE}

# I want to group exercises together for data logged within a certain time frame of the next/previous item
# e.g. logs within 20 minutes of each other, but not further apart than that
# This would let me break up the minute-by-minute data into discrete "sessions"

max_time_diff <- minutes(10)
min_workout_duration <- 15
intensity_mapping <- c("Fairly Active" = 1, "Very Active" = 2)

workouts_src_workout_wide <- intensity_src_mins_tall %>%
  filter(intensity == "Fairly Active" | intensity == "Very Active") %>%
  arrange(activity_minute) %>%
  group_by(id) %>%
  mutate(workout = cumsum(c(TRUE, diff(activity_minute) > max_time_diff)),
         intensity_numeric = as.numeric(
           factor(
             intensity,
             levels = names(intensity_mapping),
             labels = intensity_mapping
             )
           )
         ) %>%
  group_by(id, workout) %>%
  summarize(workout_start = min(activity_minute),
            workout_end = max(activity_minute),
            workout_duration = difftime(workout_end, workout_start, units="mins"),
            workout_mean_intensity = round(mean(intensity_numeric), digits=0)) %>%
  filter(workout_duration > min_workout_duration)
```

Now that the individual workouts have been identified, I can summarize them for each user. This allows me to plot average weekly sessions:

```{r viz_session_counts_total, eval=FALSE}
weeks_in_sample = 30 / 7 # 30 days in dataset
workouts_sum_id_wide <- workouts_src_workout_wide %>%
  group_by(id) %>%
  summarize(fairly_active_sessions = sum(workout_mean_intensity == 1),
            very_active_sessions = sum(workout_mean_intensity == 2)) %>%
  mutate(total_sessions = fairly_active_sessions + very_active_sessions) %>%
  mutate(average_weekly_sessions = ceiling(total_sessions / weeks_in_sample))

session_count_cumulative <- workouts_sum_id_wide %>%
  select(id, average_weekly_sessions) %>%
  arrange(average_weekly_sessions)

total_users <- nrow(session_count_cumulative)
session_count_cumulative <- session_count_cumulative %>%
  mutate(cumulative = row_number() / total_users)

ymax <- 5
ggplot(session_count_cumulative, aes(x=average_weekly_sessions)) +
  geom_histogram(binwidth = 1, col="white") +
  scale_x_continuous(breaks = seq(0, 20, by = 1)) +
  geom_line(aes(x = average_weekly_sessions,
                y = cumulative * ymax),
            col="red") +
  scale_y_continuous(name = "Number of Users",
                     sec.axis = sec_axis(~./ymax,name = "Cumulative Percentage of Users")) +
  labs(title = "Average Weekly Sessions",
       x = "Average Weekly Sessions")
```

Findings: most people are averaging 5 "sessions" or less of intense activity per week.

```{r viz_workouts_by_duration, eval=FALSE}
ggplot(workouts_src_workout_wide, aes(x=workout_duration)) +
  geom_histogram(bins=20)
```

TODO: Analyse the two outliers

When identifying the individual workout sessions, I also analysed the intensity data to determine an average intensity for each session. 

Question: Are users more likely to engage in Fairly or Very Active activities when exercising?

To answer this, I summarized the intensity data on a per-ID basis, to get separate totals for sessions that averaged Fairly Active and Very Active, respectively.

```{r viz_session_counts_by_intensity, eval=FALSE}
session_count_long <- workouts_sum_id_wide %>%
  select(id, fairly_active_sessions, very_active_sessions) %>%
  tidyr::gather(key = "intensity", value = "count", -id) %>%
  mutate(intensity = factor(intensity, levels = c("fairly_active_sessions", "very_active_sessions")))

# Reorder session_count_long%id by "Very Active Session" count

workouts_sum_id_wide <- workouts_sum_id_wide %>%
  mutate(proportion_very_active = very_active_sessions / total_sessions) %>%
  arrange(proportion_very_active)

session_count_long$id <- factor(
  session_count_long$id,
  levels = workouts_sum_id_wide$id)

# Plot stacked bar-graph for sessions, similar to the one for overall time

ggplot(session_count_long, aes(x = id, y = count, fill=intensity)) +
  geom_bar(stat = "identity",
           position = "fill") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Session Counts by Intensity",
       x = "User ID",
       y = "Sessions")
```

Finding: Visual inspect shows a wide range

```{r viz_session_proportion, eval=FALSE}
workouts_sum_id_wide <- workouts_sum_id_wide %>%
  mutate(proportion_very_active = very_active_sessions / total_sessions)

bin_width_pct <- 0.2
ggplot(workouts_sum_id_wide, aes(x = proportion_very_active)) +
  geom_histogram(binwidth = bin_width_pct,
                 col = "white") +
  scale_x_continuous(breaks = seq(0, 1.0, bin_width_pct)) +
  scale_y_continuous(breaks = seq(0,17,by=1)) +
  labs(title = "Proportion of Very Active Sessions",
       x = "Proportion of Very Active Sessions (%)",
       y = "Count of Users")
```

```{r viz_workouts_by_intensity, eval=FALSE}
ggplot(workouts_src_workout_wide, aes(x=workout_mean_intensity)) +
  geom_histogram(binwidth = 1, col = "white") +
  scale_x_continuous(
    breaks = seq(1, 2, by = 1),
    labels = c("Fairly Active", "Very Active"))
```

Findings:

* The vast majority of non-sedentary times are spent in Lightly Active Mode
* The remaining Fairly and Very Active time is pretty evenly spread, with 15 users mostly Fairly Active, and 16 users mostly Very Active
* Majority of users do 5 sessions per week or fewer on average

Conclusions: users aren't 


#### TO OMIT: Why Bellabeat doesn't need MET tracking

All of the most-used features from the database are available on the Bellabeat Ivy except MET tracking. The MET (Metabolic Equivalent of Task) is a unit of measurement for energy expenditure, or exertion, intended to aid direct comparison between different types of exercise. The Ivy does not generate MET data, but does track calories burnt, which can also be used as a measure of exertion. I recommend presenting calorie tracking as a suitable alternative when targeting advertising at those customers who use the MET tracking feature.

Note that the two features are nearly equivalent and may not present any meaningful distinction to customers.

### TODO: Step Counts

* Step count vs distance and intensity: are people running? Walking?
* Do people hit their 10,000 steps? (Yes, I know that's an arbitrary figure)
* Do people increase their step counts over time?



## TODO: Analysing the lesser-used features

### TODO: Body Composition

* What body types are being logged?
* Does anything change over time?

### TODO: Heart Rate

* 

### TODO: Sleep quality

* Are people who track sleep sleeping well?
* Does their sleep quality change over time at all?

#### Display data for first-glance analysis

``` {r, data_display, include=FALSE, eval=FALSE}
selected_Id <- "1503960366"
sleepDay_merged_oneId <- sleepDay_merged %>%
  filter(Id == selected_Id)
  
ggplot(data = sleepDay_merged_oneId) +
  geom_point(
    mapping = aes(
      x = SleepDay,
      y = TotalMinutesAsleep,
      colour = Id
    )
  )

sleep_src_mins_tall_oneId <- sleep_src_mins_tall %>%
  filter(Id == selected_Id)
  
ggplot(data = sleep_src_mins_tall_oneId) +
  geom_point(
    mapping = aes(
      x = date,
      y = value,
      colour = Id
    )
  )
```





## TODO: Ivy-first method

Use the advertised features of the Ivy to guide the analysis

### Plotting Functions
```{r util_functions}
round_data_to_bin <- function(data, bin_width) {
  rounding_factor <- 1 / bin_width
  return(round(data * rounding_factor) / rounding_factor)
}

get_histogram_max_count <- function(data, bin_width) {
  rounding_factor <- 1 / bin_width
  # Round data to nearest multiple of bin_width
  data_rounded <- round_data_to_bin(data, bin_width)
  max_count <- max(table(data_rounded))
  return(max_count)
}

rescale_plot <- function(p, x_min = 0, x_max = 10, x_step = 1, y_min = 0, y_max = 10, y_step = 1) {
  p <- p +
    scale_x_continuous(breaks = seq(x_min, x_max, by=x_step),
                       labels = scales::comma_format(),
                       limits=c(x_min, x_max)) +
    scale_y_continuous(breaks = seq(y_min, y_max, by=y_step),
                       labels = scales::comma_format(),
                       limits=c(y_min, y_max))
  return(p)
}
```


```{r plot_histo_pareto}
plot_histo_pareto <- function (data, bin_width) {
  # Cumulative data uses same bins as histogram
  data <- sort(data)
  data_rounded <- round_data_to_bin(data, bin_width)
  
  # Calculate highest count so axes can be adjusted
  highest_count <- get_histogram_max_count(data, bin_width)
  data_pareto <- seq(1, length(data), by = 1) / length(data) * highest_count

  # Calculate stats for overlay on histogram
  data_max <- max(data)
  data_mean <- round(mean(data), digits = 2)
  data_median <- round(median(data), digits = 2)
  sum_stats <- data.frame(Statistics = c("Mean", "Median"),
                          value = c(data_mean, data_median))
  label_text <- paste("Mean =", data_mean, "\nMedian =", data_median)

  p <- ggplot() +
    geom_histogram(aes(x = data),
                   color = "white",
                   binwidth = bin_width) +
    geom_line(aes(x = data_rounded,
                  y = data_pareto),
                  color="forestgreen") +
    geom_vline(data = sum_stats,
               aes(xintercept = value,
                   linetype = Statistics,
                   color = Statistics),
               size = 1) +
    scale_x_continuous(breaks = seq(0, data_max + bin_width, by = bin_width)) +
    scale_y_continuous(name = "Count",
                       breaks = seq(0, highest_count, by = 1),
                       sec.axis = sec_axis(~./highest_count, name = "Cumulative Percentage of Count")) +
    annotate("text", x = Inf, y = Inf, label = label_text,
           hjust = 1, vjust = 1, size = 4) +
    labs(x = "Value")
  
  return(p)
}
```

``` {r plot_time_coefficients}
get_time_coefficients <- function(ids, timestamps, values) {
  tbl <- tibble(
    id = ids,
    timestamp = timestamps,
    value = values
  )

  coeffs <- tbl %>%
    mutate(day_of_year = yday(timestamp)) %>%
    group_by(id) %>%
    summarize(correlation = cor(day_of_year, value))

  return(coeffs)
}

plot_time_coefficients <- function(coeffs) {
  bin_width <- 0.1
  max_count <- get_histogram_max_count(coeffs$correlation, bin_width)
  ggplot(coeffs, aes(x = correlation)) +
    geom_histogram(binwidth = bin_width, color = "white") +
    scale_x_continuous(breaks = seq(-1.0, 1.0, by=bin_width)) +
    scale_y_continuous(breaks = seq(0, max_count, by=1)) +
    labs(title = "Correlations with Time",
         caption = "Positive values imply variable of interest generally increased over time.",
         x = "Coefficient of Correlation",
         y = "Count")
}

get_time_coefficients_plot <- function(ids, timestamps, values) {
  coeffs <- get_time_coefficients(ids, timestamps, values)
  p <- plot_time_coefficients(coeffs)
  return(p)
}
```

```{r plot_wide_data_to_stacked_bar}
wide_to_stacked_bar_plot <- function(data_wide, key, value, key_order) {
  if(!("id" %in% colnames(data_wide))) {
    print("ERROR: data does not include \"id\" column: cannot convert.")
  } else {
    # Convert the data from wide to long. Set the factor levels to control the stacking order of the bars
    data_long <- data_wide %>%
      tidyr::gather(key = !!sym(key), value = !!sym(value), -id) %>%
      mutate(!!sym(key) := factor(!!sym(key), levels = key_order))

    # Order IDs in wide data based on value of first key, then rearrange long data.
    # This ensures the resultant plot sorts the IDs in ascending order of the first key
    first_key <- key_order[1]
    data_wide <- data_wide %>% arrange(!!sym(first_key))
    data_long$id <- factor(
      data_long$id,
      levels = data_wide$id)
    
    # Plot graph with angled ID labels
    p <- ggplot(data_long, aes(x= id, y= !!sym(value), fill= !!sym(key))) +
      geom_bar(stat = "identity") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    # scale_y_continuous(breaks = seq(0, 24, by = 1))
  }
}
```

```{r plot_scatter_with_LOBF}
scatter_with_LOBF <- function(data_x, data_y, labels) {
  data <- tibble(
    x = data_x,
    y = data_y
  )
  
  p <- ggplot(data,
              aes(x = x, y = y)) +
    geom_point() +
    geom_smooth(method = "lm",
                se = FALSE,
                color = "blue") +
    stat_cor(mapping=aes(label=..rr.label..),
             method="pearson",
             label.x=-Inf,
             label.y=Inf,
             hjust = -0.1,
             vjust = 1.1) +
    labs(title = paste(labels[1], "vs.", labels[2]),
         x = labels[1],
         y = labels[2])
  return(p)
}
```

### Readiness score

* Point-of-difference feature
* Check when people are wearing their fitbits
  * Need to know if they're wearing them at night ("While you sleep at night and your body is in its calmest state, Ivy measures your resting heart rate, respiratory rate, and cardiac coherence.")

#### Do people sleep with their FitBits on?

```{r night_usage_heartrate}
# Find nights where enough heart-rate data was logged to imply the user slept with their FitBit on

sec2hour <- 1 / 3600
# Sleep range set to between 10pm and 6am
sleep_range_stt_hour <- 22
sleep_range_end_hour <- 6
# Anything more than 60 seconds between logs is considered a removal (typical poll rate is 1-5 seconds)
max_poll_gap_secs <- 60
# Minimum six hours must be logged to be counted
min_hours_logged <- 6
nights_total <- 33

nights_logged_by_user <- heartrate_src_seconds_tall %>%
  # For logs in the AM, the night-of is set to the day before
  mutate(night_of_yday = ifelse(hour(heart_rate_second) >= sleep_range_stt_hour, yday(heart_rate_second),
                                ifelse(hour(heart_rate_second) < sleep_range_end_hour, yday(heart_rate_second) - 1,
                                       NA))) %>%
  # Only consider logs from the "sleep" range
  filter(!is.na(night_of_yday)) %>%
  # For each user, determine the time between polls during the night-time
  group_by(id) %>%
  mutate(time_since_last_poll = as.double(difftime(heart_rate_second, lag(heart_rate_second), units = "secs"))) %>%
  # Remove any logs that are followed by too long a gap
  filter(time_since_last_poll <= max_poll_gap_secs) %>%
  group_by(id, night_of_yday) %>%
  # Find the total time logged as the sum of time between valid logs
  summarize(logged_time_hours = sum(time_since_last_poll) * sec2hour) %>%
  # Count up the nights where each user logged sufficient time to be considered asleep with their fitbit
  group_by(id) %>%
  summarize(nights_logged = sum(logged_time_hours > min_hours_logged),
            nights_logged_pct = nights_logged / nights_total)

# Compare nightly usage amongst HR-users to average exercise intensity
average_intensities <- activity_sum_days_wide %>%
  mutate(total_active_minutes = minutes_fairly_active + minutes_very_active) %>%
  group_by(id) %>%
  summarize(average_active_minutes = mean(total_active_minutes))

nights_logged_vs_intensity <- nights_logged_by_user %>%
  merge(average_intensities, by = "id")

ggplot(nights_logged_vs_intensity) +
  geom_point(aes(x = average_active_minutes, y = nights_logged_pct))

correlation <- cor(nights_logged_vs_intensity$average_active_minutes, 
                   nights_logged_vs_intensity$nights_logged_pct)

# Most users do not have heart-rate tracking: add their IDs back in for the final analysis
all_ids <- unique(activity_sum_days_wide$id)
ids_to_append <- setdiff(all_ids, nights_logged_by_user$id)
new_rows <- data.frame(
  id = ids_to_append,
  nights_logged = 0,
  nights_logged_pct = 0
)
nights_logged_by_user <- bind_rows(nights_logged_by_user, new_rows)

# Plot histogram showing who logged what fraction of their nights
plot_bin_width <- 0.25
ggplot(nights_logged_by_user) +
  geom_histogram(aes(x = nights_logged_pct),
                 color = "white",
                 binwidth = plot_bin_width) + 
  scale_x_continuous(breaks = seq(0,1,by=plot_bin_width)) +
  scale_y_continuous(breaks = seq(0,33,by=1)) +
  labs(title = "Percentage of Nights Logged by Heart-rate Sensor",
       x = "Percentage of Nights",
       y = "Count of Users")
```

Findings:

* 76% of users did not log any heartrate data overnight
* 18% of users logged heartrate data on a majority of nights
* Amongst users with heart-rate logging, there was only a loose relationship between average intensity of activity and nightly logging (R^2 = 0.213), so I can't say with confidence that people tracking overnight are more exercise-conscious
* There's no clear reason why people are not wearing their FitBits overnight

Recommendations:

Most people who have heart-rate tracking don't sleep with their FitBits on. This makes sense given there doesn't appear to be any use for the heart-rate data while a person is sleeping: it's mainly used to enable exercise intensity tracking
* The Readiness Score can be marketed as a value-add feature that does more with the heart-rate data than competing products
* Given the feature requires people to wear the device overnight, it would be worthwhile to try to gather more information about why people don't already do this. For users without a device with heart-rate tracking this makes sense, but for those who do have it, the reason for not using it may be negative, e.g. the device is not comfortable, the strap causes irritation, etc. Understanding these reasons would help target the advertising further.
* The non-allergenic materials used in the Ivy can be used to highlight the comfort aspects of the device when marketing features that require overnight wearing.

### Heart Rate Tracking and Activity Tracking

* Heart-rate Tracking: "Use it to track your workout progress and optimize personal training routines."
* Exercise Tracking: "Ivy will recognize your activity during the day, help you track up to 80 types of activity, count your steps, and discover how all that affects your body."

* Do people count their steps? Yeah, all but three of them did
* Do people engage in different types of activity? Yeah, big spread of Fairly/Very Active intensity levels, safe to say people don't all work out the same, so the more activity tracking the merrier

These two features are closely related, since one of the primary functions of the HR tracking is to detect exercise intensity. Step count and distance covered can also be used to further analyse users' activity.

In order to better understand how people are using their FitBits, I analysed the amounts and intensity of different users exercise.

#### How do people exercise with their FitBits?
#### How much time do people spend exercising?

I start by getting the top-level breakdown of users time vs intensity:

```{r viz_average_daily_intensities_by_id}
# For each ID, generate averages for the three non-sedentary intensity levels
mean_daily_intensities_wide <- intensity_sum_days_wide %>%
  group_by(id) %>%
  summarize("sedentary" = mean(minutes_sedentary) / 60,
            "lightly_active" = mean(minutes_lightly_active) / 60,
            "fairly_active" = mean(minutes_fairly_active) / 60,
            "very_active" = mean(minutes_very_active) / 60)

#Convert data to long-format for plotting as a histogram
intensity_order = c("sedentary", "lightly_active", "fairly_active", "very_active")
mean_daily_intensities_long <- mean_daily_intensities_wide %>%
  tidyr::gather(key = "intensity", value = "mean_hours", -id) %>%
  mutate(intensity = factor(intensity, levels = intensity_order))

# Reorder IDs in order of "Very Active" time
mean_daily_intensities_wide <- mean_daily_intensities_wide %>%
  mutate("total_active" = lightly_active + fairly_active + very_active) %>%
  arrange(total_active)

# Apply order of IDs to long-format data to force plot order
mean_daily_intensities_long$id <- factor(
  mean_daily_intensities_long$id,
  levels = mean_daily_intensities_wide$id)

ggplot(mean_daily_intensities_long, aes(x= id, y= mean_hours, fill= intensity)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(breaks = seq(0, 24, by = 1)) +
  labs(title="Average Time by Intensity Zone",
       x = "User ID",
       y = "Total Average Daily Time (hours)")
```

Findings:

* The large majority of people's time is clearly being spent in a sedentary state.
* This makes sense, since it includes sleep tracking as well as time spent awake but immobile
* There is a wide spread of times spent non-sedentary, with no clear outliers at this point
* The large majority of non-sedentary time is Lightly Active

Given how large the majority of non-Sedentary time is "Lightly Active", and based on our understanding of the Intensity zones from previous sections, I will assume "Lightly Active" time includes any activity more intense than sitting down, up to an including activities like walking for leisure. Anything more intense would therefore fall into the "Fairly Active" or "Very Active" categories. With this in mind, I'll proceed with the assumption that the "Fairly Active" and "Very Active" zones represent intentional exercise.

Next, I analyse users time spent intentionally exercising:

```{r viz_active_hours_by_count}
# Update total_active to reflect new definition
mean_daily_intensities_wide <- mean_daily_intensities_wide %>%
  mutate(total_active = fairly_active + very_active) %>%
  arrange(total_active)

mean_active_hours <- mean_daily_intensities_wide %>%
  select(id, total_active)

# Add cumulative user count for Pareto analysis
total_users <- nrow(mean_active_hours)
bin_width <- 0.25
rounding_factor <- 1 / bin_width
mean_active_hours <- mean_active_hours %>%
  mutate(total_active_rounded = round(total_active * rounding_factor) / rounding_factor) %>%
  mutate(cumulative_users = row_number() / total_users)

mean_hours <- mean(mean_active_hours$total_active)
median_hours <- median(mean_active_hours$total_active)

ymax <- 12
xmax <- 13.5
ggplot(mean_active_hours, aes(x=total_active)) +
  geom_histogram(binwidth = bin_width, col="white") +
  scale_x_continuous(breaks = seq(0, xmax, by = bin_width)) +
  geom_line(aes(x = total_active_rounded,
                y = cumulative_users * ymax),
            col="red") +
  geom_vline(aes(xintercept = mean_hours, col = 'red')) +
  geom_vline(aes(xintercept = median_hours), col = 'blue') +
  scale_y_continuous(name = "Number of Users",
                     breaks = seq(0, ymax, by = 1),
                     sec.axis = sec_axis(~./ymax,name = "Cumulative Percentage of Users")) +
  labs(title = "Active Hours per Week",
       x = "Hours")
```

Findings:

* 72.7% of the cohort (24 users) get less than 45 minutes of Active Time per week on average.
* 18% of the cohort (6 users) do more than 1 hour a week on average.
* Most people in the dataset are not doing much more than very light exercise.

#### How intensely do people exercise? 

Here I analyse the data to see if there is any variation in the level of intensity of exercise between users. The variation was analysed by examining the ratio of "Fairly Active" to "Very Active" time for each user:

```{r viz_proportion_intensity}
# For each ID, generate proportion Very Active time
mean_daily_intensities_wide <- mean_daily_intensities_wide %>%
  mutate(proportion_very_active = very_active / total_active)

# Convert data to long-format for plotting as a histogram
intensity_order = c("fairly_active", "very_active")
mean_daily_intensities_long <- mean_daily_intensities_wide %>%
  tidyr::gather(key = "intensity", value = "mean_hours", -id) %>%
  mutate(intensity = factor(intensity, levels = intensity_order)) %>%
  filter(intensity == "fairly_active" | intensity == "very_active")

# Reorder IDs in order of "Very Active" time
mean_daily_intensities_wide <- mean_daily_intensities_wide %>%
  arrange(proportion_very_active)

# Apply order of IDs to long-format data to force plot order
mean_daily_intensities_long$id <- factor(
  mean_daily_intensities_long$id,
  levels = mean_daily_intensities_wide$id)

ggplot(mean_daily_intensities_long, aes(x= id, y= mean_hours, fill= intensity)) +
  geom_bar(stat = "identity",
           position = "fill") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Proportion of Average Time by Intensity Zone",
       x = "User ID",
       y = "Average Daily Time (%)")
```

Findings: 

* It's quite evenly spread out from around 10% Very Active to around 90% Very Active across all users, with no clear outliers.
* It would appear that the user group engages in a range of exercise activities of varying intensity.

#### Do people who track more do more-intense exercise?

The proportion of Active Time spent Very Active was plotted against the total Active Time to see if there was a correlation. From the earlier analysis of overall Active Hours per week, we can see that Active Hours are skewed right and concentrated towards the lower end: given this distribution, I also re-ran the analysis twice, once with the upper quintile of users removed, and once with the lower quintile removed. The results are plotted below.

```{r viz_logged_minutes_vs_intensity}
plot_all_data <- ggplot(mean_daily_intensities_wide, aes(x=total_active, y=proportion_very_active)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "blue") +
  stat_cor(aes(label=..rr.label..),
           label.x=0,
           label.y=0) +
  labs(title="Proportion Very Active vs. Total Active Time",
       x = "Average Weekly Active Time (hours)",
       y = "Proportion of Very Active Time (%)")

data <- mean_daily_intensities_wide %>%
  filter(total_active <= 1.25) # id != "3977333714")

plot_no_upper_quint <- ggplot(data, aes(x=total_active, y=proportion_very_active)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "blue") +
  stat_cor(aes(label=..rr.label..),
           label.x=0,
           label.y=0) +
  labs(title="Proportion Very Active vs. Total Active Time (Highest Quintlie Removed)",
       x = "Average Weekly Active Time (hours)",
       y = "Proportion of Very Active Time (%)")

data <- mean_daily_intensities_wide %>%
  filter(total_active > 0.25) # id != "3977333714")

plot_no_lower_quint <- ggplot(data, aes(x=total_active, y=proportion_very_active)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "blue") +
  stat_cor(aes(label=..rr.label..),
           label.x=0,
           label.y=0) +
  labs(title="Proportion Very Active vs. Total Active Time (Lowest Quintile Removed)",
       x = "Average Weekly Active Time (hours)",
       y = "Proportion of Very Active Time (%)")

rm(data)

plot_all_data <- plot_all_data + 
  scale_x_continuous(limits=c(0,2)) + 
  scale_y_continuous(limits=c(0,1))
plot_no_upper_quint <- plot_no_upper_quint + 
  scale_x_continuous(limits=c(0,2)) + 
  scale_y_continuous(limits=c(0,1))
plot_no_lower_quint <- plot_no_lower_quint + 
  scale_x_continuous(limits=c(0,2)) + 
  scale_y_continuous(limits=c(0,1))

grid.arrange(plot_all_data, plot_no_upper_quint, plot_no_lower_quint, ncol = 3)
```

Findings: 
* Very Active Proportion was positively correlated with Active time, with an R^2 value of 0.27.
* Removing the upper quintile decreased the correlation to 0.23, indicating that those doing less exercise also do a wider range of intensities
* Removing the lower quintile increased the correlation to 0.32, indicating that those doing the most exercise are doing a narrower range of higher-intensity activities.
* Overall, the results indicate that the majority of the cohort is engaged in a wide range of activities. There is a large subset of the group doing a low amount of varied exercise, as well as a small group of outliers doing a higher amount of more-intense exercise.

#### Do people prefer to walk or run?

Why do I care: If people like to run, we can market it at those people

How do we find out if people run?
Steps/second
Distance/second
How will we know if they're running? Speed above a certain level

I can use the src logs directly as they are logged every minute, so each data point is also an average speed in steps per minute. From this, I can set up some ranges for different walking types based on speed, count the number of steps logs in each range for each person/day, then finally average the sums over the days to get each users' average time spent at each speed

```{r running_analysis}
brisk_walk_thld <- 80
running_thld <- 110

average_movement_times_daily <- steps_src_mins_tall %>%
  mutate(activity_day = as.POSIXct(format(as.Date(activity_minute)))) %>%
  group_by(id, activity_day) %>%
  summarize(not_walking = sum(steps == 0) / 60,
            moderate_walking = sum(steps > 0 & steps <= brisk_walk_thld) / 60,
            brisk_walking = sum(steps > brisk_walk_thld & steps <= running_thld) / 60,
            running = sum(steps > running_thld) / 60)

average_movement_times <- average_movement_times_daily %>%
  group_by(id) %>%
  summarize(not_walking = mean(not_walking),
            moderate_walking = mean(moderate_walking),
            brisk_walking = mean(brisk_walking),
            running = mean(running)) %>%
  arrange(running)

speed_order = c("not_walking", "moderate_walking", "brisk_walking", "running")
average_movement_times_long <- average_movement_times %>%
  tidyr::gather(key = "speed", value = "mean_hours", -id) %>%
  mutate(speed = factor(speed, levels = speed_order))

average_movement_times_long$id <- factor(
  average_movement_times_long$id,
  levels = average_movement_times$id
)

ggplot(average_movement_times_long %>% filter(speed != "not_walking"),
       aes(x = id, y = mean_hours, fill = speed)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(breaks = seq(0,33,by=1)) +
  labs(title = "Average Daily Running Speeds by User ID",
       x = "User ID",
       y = "Average Daily Hours")

plot_mod_walking <- plot_histo_pareto(average_movement_times$moderate_walking,
                                      bin_width = 0.25) +
  labs(title = "Average Moderate Walking Hours per Week",
       x = "Average Hours per Week")
print(plot_mod_walking)

plot_brisk_walking <- plot_histo_pareto(average_movement_times$brisk_walking,
                                        bin_width = 0.25) +
  labs(title = "Average Brisk Walking Hours per Week",
       x = "Average Hours per Week")
print(plot_brisk_walking)

plot_running <- plot_histo_pareto(average_movement_times$running,
                                  bin_width = 0.25) +
  labs(title = "Average Running Hours per Week",
       x = "Average Hours per Week")
print(plot_running)
```

Findings: 

* Most people spend most of their time walking slowly
* Brisk walking is somewhat more common, with 24 of 33 users logging between 0.25 and 0.5 hours per week
* Runners are rare, with 30 of 33 users logging 0.25 hours or less per week, and the remaining three outliers logging between 0.5 and 1.25 hours per week

#### Do people increase their active walking hours over time?

```{r active_walking_over_time}
p_mod_walking_vs_time <- get_time_coefficients_plot(average_movement_times_daily$id,
                                                    average_movement_times_daily$activity_day,
                                                    average_movement_times_daily$moderate_walking) +
  labs(title = "Moderate Walking Over Time")

p_brisk_walking_vs_time <- get_time_coefficients_plot(average_movement_times_daily$id,
                                                    average_movement_times_daily$activity_day,
                                                    average_movement_times_daily$brisk_walking) +
  labs(title = "Brisk Walking Over Time")

p_running_vs_time <- get_time_coefficients_plot(average_movement_times_daily$id,
                                                    average_movement_times_daily$activity_day,
                                                    average_movement_times_daily$running) +
  labs(title = "Running Over Time")

grid.arrange(p_mod_walking_vs_time, p_brisk_walking_vs_time, p_running_vs_time, ncol = 3)
```

Findings:

* Most users actually decreased or did not change their walking time over the data period
* This finding was consistent across all walking types
* There is no evidence to suggest FitBit users increased their walknig times as a result of their FitBit device usage

#### Are people getting their steps in?

```{r average_daily_step_counts}
average_daily_steps <- steps_sum_days_tall %>%
  group_by(id) %>%
  summarize(average_steps = mean(steps_total))

plot_steps <- plot_histo_pareto(average_daily_steps$average_steps, 2000) +
  labs(title = "Average Daily Steps by User",
       x = "Average Daily Steps")
print(plot_steps)
```

Answer: Typically 7500.

#### Do people increase their step counts over time?

```{r coefficient_with_time}
p_steps_time_coeffs <- get_time_coefficients_plot(steps_sum_days_tall$id,
                                                steps_sum_days_tall$activity_day,
                                                steps_sum_days_tall$steps_total) +
  labs(title = "Correlation between Step Counts and Time")
print(p_steps_time_coeffs)
```

Findings:
* As with walking types, the majority of users actually did fewer steps over time

#### Do people work out inside?

If people are primarily engaging in stationary exercises, e.g. on a treadmill, exercise bike, or weights station, we should see no correlation between time spent active and distance covered. We can test this idea by plotting minutes spent active against distance logged as active.

```{r distance_vs_intensity}
distance_vs_active <- activity_sum_days_wide %>%
  select(id, activity_day,
         distance_moderately_active,
         distance_very_active,
         minutes_fairly_active,
         minutes_very_active) %>%
  mutate(distance_active = distance_moderately_active + distance_very_active,
         minutes_active = minutes_fairly_active + minutes_very_active) %>%
  group_by(id) %>%
  summarize(mean_distance_active = mean(distance_active),
            mean_minutes_active = mean(minutes_active))

p <- scatter_with_LOBF(distance_vs_active$mean_minutes_active,
                       distance_vs_active$mean_distance_active,
                       c("Active Minutes", "Active Distance")) +
  labs(caption = "\"Active\" refers to time spent either Fairly or Very Active")
print(p)
```

Findings:

* Active Minutes correlate strongly with Active distance, with R^2 = 0.77. This implies people spending more time exercising are also spending more time moving.

We can further analyse this trend by looking at what proportion of the overall distance people cover is Active Distance.

```{r distance_intensity_pct, eval=FALSE}
average_distances <- activity_sum_days_wide %>%
  group_by(id) %>%
  summarize(mean_distance = mean(distance_total))

distance_vs_intensity <- activity_sum_days_wide %>%
  group_by(id) %>%
  summarize(mean_distance_very_active = mean(distance_very_active),
            mean_distance_moderately_active = mean(distance_moderately_active),
            mean_distance_lightly_active = mean(distance_lightly_active),
            mean_distance_sedentary = mean(distance_sedentary)) %>%
  mutate(mean_distance_total = mean_distance_very_active + 
           mean_distance_moderately_active +
           mean_distance_lightly_active +
           mean_distance_sedentary) %>%
  mutate(pct_distance_very_active = mean_distance_very_active / mean_distance_total,
         pct_distance_moderately_active = mean_distance_moderately_active / mean_distance_total,
         pct_distance_lightly_active = mean_distance_lightly_active / mean_distance_total,
         pct_distance_sedentary = mean_distance_sedentary / mean_distance_total)

ggplot(distance_vs_intensity, aes(x = mean_distance_total, y = pct_distance_very_active)) +
  geom_point()

distance_vs_intensity_pct <- distance_vs_intensity %>%
  select(id,
         pct_distance_very_active,
         pct_distance_moderately_active,
         pct_distance_lightly_active,
         pct_distance_sedentary)
stack_order <- c("pct_distance_very_active",
                 "pct_distance_moderately_active",
                 "pct_distance_lightly_active",
                 "pct_distance_sedentary")

# p <- wide_to_stacked_bar_plot(distance_vs_intensity_pct, "intensity", "distance", stack_order) +
#   labs(title = "Intensity Levels as Percentage of Distance Covered")
# print(p)

histo_data <- distance_vs_intensity_pct %>%
  select(id,
         pct_distance_very_active,
         pct_distance_moderately_active) %>%
  mutate(pct_distance_active = pct_distance_very_active + pct_distance_moderately_active)
p <- plot_histo_pareto(histo_data$pct_distance_active,
                       bin_width = 0.1) +
  labs(title = "Active Distance as Percentage of Total Distance")
print(p)
```

Findings:

* On average, 31% of distance covered by the cohort is Active Distance
* Approximately one-quarter of the cohort (8, or 24%) log 50% or more of their distance as Active Distance
* These findings support the conclusion that users are engaging in both stationary and non-stationary exercise

##### Recommendations

* To appeal to the FitBit market, the marketing for the Ivy's exercise tracking should highlight a broad range of activities ranging from gentle to more intense. The marketing should not target highly-athletic groups, as these represent only a small part of the user base.
* The cohort does appear to engage in outdoor activities that cover long distances such as running, as opposed to stationary activities such as gym workouts, so the marketing can highlight activities like light jogging, walking dogs, and cycling
* The marketing for the Ivy should avoid any claims that users will increase their activity levels as a result of their purchasing the device, as the data does not support this claim.

### Respiratory Rate

"How you breathe says a lot about your mind and body."
* Point-of-difference feature
* I don't believe the FitBit has anything for this, so look for a proxy

### Resting Heart Rate

"Resting heart rate (RHR) is the number of heartbeats per minute, measured when the body is fully calm during the night."

* Point-of-difference feature
  * Especially if people *aren't* wearing their devices at night

### Cardiac Coherence

"The cardiac coherence parameter shows how your heart rate variability and breathing rate are synchronized."

* Point-of-difference feature

### Wellness Score

"Ivy monitors your activity, sleep, meditation, menstrual cycle, and hydration log..."

* Point-of-difference feature

### Activity Tracking

"Ivy will recognize your activity during the day, help you track up to 80 types of activity, count your steps, and discover how all that affects your body."

* Do people count their steps? Yeah, all but three of them did
* Do people engage in different types of activity? Yeah, big spread of Fairly/Very Active intensity levels, safe to say people don't all work out the same, so the more activity tracking the merrier

### Hydration

"Know how much you really drink and set water reminders. Also, factor in your unique physique, activity levels, age, height, and weight to calculate the right amount of water for your body."

* Point-of-difference feature
* That last part about "factor in your unique physique" may be more or less important to people, and that would be evidenced by how many people used bodycomp features
* No direct data to analyse existing usage for this kind of feature

### Sleep Tracking

"Ivy monitors your sleep patterns and knows exactly how your night is going to affect the next day."

* Again, first off is just how many people used the sleep-tracking feature
* Check if volume or quality of sleep improved

### Mindfulness

"Ivy helps you keep track of your mindful minutes and get insight into how much your meditative, breathing, or similar practices affect your psycho-physical well-being."

* Point-of-difference feature
* If there is a proxy for breathing, maybe I can determine if people are meditating with their fitbits, although that seems like a stretch

### Menstrual Cycle

"Ivy helps you tune in with your body’s rhythm and track your period, ovulation, pregnancy, or birth control. Monitor your changes in the menstrual cycle, log symptoms, and even predict issues before they appear."

* Point-of-difference feature

# TODO Step 5: Share

In which I present all of my key findings and their supporting visualisations

# TODO Step 6: Act

In which I summarise my recommendations

* Push hard on any features not used much as a value-add to the customer
